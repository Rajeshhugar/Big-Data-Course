from pyspark.sql import SparkSession
# Initialize Spark Session
# Initialize Spark Session with dynamic allocation
spark = SparkSession.builder \
.appName("Higher_Level_APIs") \
.getOrCreate()


spark


import os
path = r"C:\Users\hugar\Desktop\DATA_ENGINEERING\ecommerce_data\1MB\customers.csv"
print("Exists:", os.path.exists(path))



#os.chdir(path)



df = spark.read \
    .format('csv') \
    .option('header', 'true') \
    .option('inferSchema', 'true') \
    .load(path)



df





df.show(5)


df.printSchema()


df.createOrReplaceTempView('customers')


spark.sql('''select * from customers 
limit 5''').show()


spark.sql('select customer_id,city from customers limit 5').show()


spark.stop()





from pyspark.sql import SparkSession
# Initialize Spark Session
# Initialize Spark Session with dynamic allocation
spark = SparkSession.builder \
.appName("Higher_Level_APIs_2") \
.getOrCreate()





rdd = spark.sparkContext.parallelize([
    ('0','Customer_0','Banagalore','India'),
    ('1','Customer_1','Delhi','India')])


df = spark.createDataFrame(rdd,['cust_id','cust_name','state','country'])


df.show()


df.createOrReplaceTempView('customers')


spark.sql('select cust_id from customers').show()


df.printSchema()


selected_cols = df.select('cust_id','cust_name')


selected_cols.show()


spark.stop()














from pyspark.sql import SparkSession
# Initialize Spark Session
# Initialize Spark Session with dynamic allocation
spark = SparkSession.builder \
.appName("Higher_Level_APIs_3") \
.getOrCreate()


hdfs_path = '/tmp/customers_500mb.csv'



df = spark.read \
    .format('csv') \
    .load(hdfs_path)


df = spark.read \
    .format('csv') \
    .option('inferSchema', 'true') \
    .option('header', 'true') \
    .load(hdfs_path)


df_header_only = spark.read \
    .format('csv') \
    .option('header', 'true') \
    .load(hdfs_path)


from pyspark.sql.types import *

schema = StructType([
StructField("customer_id", IntegerType(), True),
StructField("name_of_customer", StringType(), True),
StructField("city", StringType(), True),
StructField("state", StringType(), True),
StructField("country", StringType(), True),
StructField("registration_date", StringType(), True),
StructField("is_active", BooleanType(), True),
])



df_explicit = spark.read \
.format("csv") \
.option("header", "false") \
.schema(schema) \
.load (hdfs_path)



df_explicit.show(5)


df_4 = spark.read \
    .format('csv') \
    .option('inferSchema', 'true') \
    .option('header', 'true') \
    .load(hdfs_path)


df_5 = spark.read \
    .format('csv') \
    .option('inferSchema', 'true') \
    .option('header', 'true') \
    .option('samplingRatio',0.1)\
    .load(hdfs_path)





from pyspark.sql.types import *

schema = StructType([
StructField("customer_id", IntegerType(), True),
StructField("name_of_customer", StringType(), True),
StructField("city", StringType(), True),
StructField("state", StringType(), True),
StructField("country", StringType(), True),
StructField("registration_date", StringType(), True),
StructField("is_active", BooleanType(), True),
])



ddl_schema = " customer_id INT NOT NULL, name INT, city STRING, state STRING,country STRING, registration_date TIMESTAMP,is_active BOOLEAN"


df_ddl_explicit = spark.read \
.format("csv") \
.option("header", "true") \
.schema(ddl_schema) \
.load (hdfs_path)



df_ddl_explicit.show(5)


df_ddl_explicit.printSchema()
