from pyspark.sql import SparkSession
# Initialize Spark Session
# Initialize Spark Session with dynamic allocation
spark = SparkSession.builder \
.appName("ReduceByKey_300mb") \
.getOrCreate()


!hadoop fs -ls -h /tmp


!hadoop fs -head /tmp/customers.csv


hdfs_path = "/tmp/customers.csv"
local_path = r"C:\Users\hugar\Desktop\DATA_ENGINEERING\ecommerce_data\1MB\customers.csv"
rdd = spark.sparkContext.textFile(local_path)





header = rdd.first()


rdd_no_header = rdd.filter(lambda row:row !=header).map(lambda row:row.split(','))


rdd_no_header.first()


key_value_rdd = rdd_no_header.map(lambda row:(row[2],1))


reduced_rdd = key_value_rdd.reduceByKey(lambda x,y : x+y)


reduced_rdd.collect()


spark.stop()








from pyspark.sql import SparkSession
# Initialize Spark Session
# Initialize Spark Session with dynamic allocation
spark = SparkSession.builder \
.appName("GroupByKey_300mb_2") \
.getOrCreate()


hdfs_path = "/tmp/customers.csv"

rdd = spark.sparkContext.textFile(hdfs_path)


header = rdd.first()
rdd_no_header = rdd.filter(lambda row:row !=header).map(lambda row:row.split(','))


key_value_rdd = rdd_no_header.map(lambda row:(row[2],1))


grouped_rdd = key_value_rdd.groupByKey()


result = grouped_rdd.map(lambda x : (x[0],len(x[1])))


result.collect()


[('Delhi', 661025),
 ('Pune', 660737),
 ('Kolkata', 660174),
 ('Chennai', 660249),
 ('Bangalore', 661013),
 ('Mumbai', 661241),
 ('Hyderabad', 662281),
 ('Ahmedabad', 660218)]


spark.stop()








spark = SparkSession.builder \
.appName ("Repartition_vs_Coalesce") \
.config("spark.executor.memory", "2g") \
.config("spark.executor.cores", "2") \
.getOrCreate()


hdfs_path = "/tmp/customers.csv"

rdd = spark.sparkContext.textFile(hdfs_path)


rdd.getNumPartitions()


repartitioned_rdd = rdd.repartition(4)


repartitioned_rdd.getNumPartitions()


reparitioned_less_rdd = rdd.repartition(2)


reparitioned_less_rdd.getNumPartitions()


reparitioned_more_rdd = rdd.repartition(200)


reparitioned_more_rdd.getNumPartitions()


reparitioned_more_rdd.collect()


coalesce_rdd = reparitioned_more_rdd.coalesce(4)


coalesce_rdd.getNumPartitions()


coalesce_rdd.count()


spark.stop()
