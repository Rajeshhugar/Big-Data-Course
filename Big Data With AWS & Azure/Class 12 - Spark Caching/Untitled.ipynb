{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Managed vs External Tables in Spark\n",
    "\n",
    "This notebook explains **Managed vs External Tables in Spark** using an example dataset (`customers_1mb.csv`). It also demonstrates how to configure the **default warehouse path** and verify tables using **Hive Metastore**.\n",
    "\n",
    "## üîπ Key Concepts\n",
    "- **Managed Table**: Spark fully controls the **data and metadata**.\n",
    "- **External Table**: Spark manages only **metadata**, while the data remains **outside** Spark's control.\n",
    "- **Hive Metastore**: Stores **table definitions** to enable SQL-like querying in Spark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 1: Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ‚úÖ Step 2: Define warehouse directory\n",
    "warehouse_location = '/tmp/mera/warehouse'\n",
    "\n",
    "# ‚úÖ Step 3: Create Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('ManagedVsExternalTables') \\\n",
    "    .config('spark.sql.warehouse.dir', warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session created with warehouse location: {warehouse_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Checking Current Warehouse Directory\n",
    "You can verify the current **Spark SQL warehouse directory** using the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/spark-warehouse'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the configured warehouse directory\n",
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Loading the Dataset\n",
    "Now, we load the **customers_1mb.csv** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- registration_date: timestamp (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load CSV data into a DataFrame\n",
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header', 'True') \\\n",
    "    .option('inferSchema', 'True') \\\n",
    "    .load('/tmp/customers_1mb.csv')\n",
    "\n",
    "# Display DataFrame schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Creating a Temporary View\n",
    "We'll create a **temporary view** to allow SQL-like queries before creating tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "|customer_id|      name|     city|      state|country|  registration_date|is_active|\n",
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n",
      "|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n",
      "|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n",
      "|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n",
      "|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n",
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view for querying\n",
    "df.createOrReplaceTempView(\"temp_customers\")\n",
    "\n",
    "# Query the view\n",
    "spark.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèó Creating a **Managed Table**\n",
    "- Spark **stores** the data inside the **warehouse directory** (`/tmp/mera/warehouse`).\n",
    "- If you **drop** this table, the **data is also deleted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "25/02/01 17:20:04 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "25/02/01 17:20:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Managed table 'managed_customers' created.\n"
     ]
    }
   ],
   "source": [
    "# Creating a Managed Table\n",
    "spark.sql(\"DROP TABLE IF EXISTS managed_customers\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE managed_customers AS \n",
    "    SELECT * FROM temp_customers\n",
    "\"\"\")\n",
    "print(\"‚úÖ Managed table 'managed_customers' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 17:21:38 WARN HiveExternalCatalog: Request to alter database default is a no-op because the provided database properties are the same as the old ones. Hive does not currently support altering other database fields.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Hive 2.3.9 does not support altering database location",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22272/2718571699.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ALTER DATABASE default SET LOCATION 'file:/tmp/mera/warehouse'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Hive 2.3.9 does not support altering database location"
     ]
    }
   ],
   "source": [
    "spark.sql(\"ALTER DATABASE default SET LOCATION 'file:/tmp/mera/warehouse'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                |comment|\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|customer_id                 |int                                                      |null   |\n",
      "|name                        |string                                                   |null   |\n",
      "|city                        |string                                                   |null   |\n",
      "|state                       |string                                                   |null   |\n",
      "|country                     |string                                                   |null   |\n",
      "|registration_date           |timestamp                                                |null   |\n",
      "|is_active                   |boolean                                                  |null   |\n",
      "|                            |                                                         |       |\n",
      "|# Detailed Table Information|                                                         |       |\n",
      "|Database                    |default                                                  |       |\n",
      "|Table                       |managed_customers                                        |       |\n",
      "|Owner                       |root                                                     |       |\n",
      "|Created Time                |Sat Feb 01 17:20:05 UTC 2025                             |       |\n",
      "|Last Access                 |UNKNOWN                                                  |       |\n",
      "|Created By                  |Spark 3.3.2                                              |       |\n",
      "|Type                        |MANAGED                                                  |       |\n",
      "|Provider                    |hive                                                     |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1738430407]                       |       |\n",
      "|Statistics                  |1219563 bytes                                            |       |\n",
      "|Location                    |hdfs://my-cluster-m/user/hive/warehouse/managed_customers|       |\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended managed_customers').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Verify Managed Table\n",
    "Run the following command in a **Dataproc terminal** to check where the managed table is stored:\n",
    "```bash\n",
    "!hdfs dfs -ls /tmp/mera/warehouse/managed_customers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Creating an **External Table**\n",
    "- The data **remains** in `/tmp/customers_1mb.csv`.\n",
    "- If you **drop** this table, the **data is not deleted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an External Table\n",
    "spark.sql(\"DROP TABLE IF EXISTS external_customers\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE external_customers \n",
    "    USING CSV \n",
    "    LOCATION '/tmp/customers_1mb.csv'\n",
    "\"\"\")\n",
    "print(\"‚úÖ External table 'external_customers' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Verify External Table\n",
    "Run the following command to check its location:\n",
    "```bash\n",
    "!hdfs dfs -ls /tmp/customers_1mb.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Verifying Tables in Hive Metastore\n",
    "You can check all available tables in Spark using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tables in Spark\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõë Dropping the Tables\n",
    "Dropping a **Managed Table** deletes both **metadata and data**, while dropping an **External Table** deletes only **metadata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop managed table (Data is deleted!)\n",
    "spark.sql(\"DROP TABLE IF EXISTS managed_customers\")\n",
    "\n",
    "# Drop external table (Data is NOT deleted!)\n",
    "spark.sql(\"DROP TABLE IF EXISTS external_customers\")\n",
    "\n",
    "print(\"‚úÖ Tables dropped successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "| Feature | Managed Table | External Table |\n",
    "|---------|--------------|----------------|\n",
    "| Data Location | Inside warehouse | Custom location |\n",
    "| Dropping Table | Deletes data | Only deletes metadata |\n",
    "| Performance | Optimized by Spark | Depends on external storage |\n",
    "\n",
    "**üöÄ Now you understand the difference between Managed and External Tables in Spark!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
