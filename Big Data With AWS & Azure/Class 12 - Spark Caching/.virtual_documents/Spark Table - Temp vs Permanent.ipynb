from pyspark.sql import SparkSession

# Initialize SparkSession with Hive support
spark = SparkSession.builder \
    .appName("TableDemo") \
    .enableHiveSupport() \
    .getOrCreate()

# Read CSV from HDFS
hdfs_path = "/tmp/customers_100.csv"  # Update path as per your HDFS location
df = spark.read.option("header", True).csv(hdfs_path)

# Show Data
df.show(5)



spark.sql('show tables').show(truncate=False)


spark.sql('drop table temp_customers_2').show()



# ---------------- Step 1: Create a Temporary Table (Session-based) ---------------- #
df.createOrReplaceTempView("temp_customers")

print("### Querying Temporary Table (Exists only in this session) ###")
spark.sql("SELECT * FROM temp_customers LIMIT 5").show()




# ---------------- Step 2: Create a Global Temporary Table (Accessible across sessions) ---------------- #
df.createOrReplaceGlobalTempView("global_customers")

print("### Querying Global Temporary Table ###")
spark.sql("SELECT * FROM global_temp.global_customers LIMIT 5").show()



spark.sql('select * from global_temp.global_customers limit 5').show()



# ---------------- Step 3: Create a Persistent Table (Stored in Hive Metastore) ---------------- #
spark.sql("DROP TABLE IF EXISTS customers_persistent")
df.write.mode("overwrite").saveAsTable("customers_persistent")

print("### Querying Persistent Table (Accessible across sessions and applications) ###")
spark.sql("SELECT * FROM customers_persistent LIMIT 5").show()



spark.sql('describe extended customers_persistent').show(truncate =False)


# Create a new session within the same Spark application
spark_new = spark.newSession()

# Verify the view exists
print("### Querying Global Temporary Table ###")
spark_new.sql("SHOW TABLES IN global_temp").show()

# Query the Global Temp View
spark_new.sql("SELECT * FROM global_temp.global_customers").show()



spark_new.sql("SELECT * FROM temp_customers LIMIT 5").show()


spark.sql('show tables').show()


spark.stop()
