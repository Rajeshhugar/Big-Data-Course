


# ✅ Step 1: Import necessary libraries
from pyspark.sql import SparkSession

# ✅ Step 2: Define warehouse directory
warehouse_location = '/tmp/mera/warehouse'

# ✅ Step 3: Create Spark session with Hive support
spark = SparkSession.builder \
    .appName('ManagedVsExternalTables') \
    .config('spark.sql.warehouse.dir', warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()

print(f"Spark session created with warehouse location: {warehouse_location}")





# Show the configured warehouse directory
spark.conf.get('spark.sql.warehouse.dir')





# Load CSV data into a DataFrame
df = spark.read \
    .format('csv') \
    .option('header', 'True') \
    .option('inferSchema', 'True') \
    .load('/tmp/customers_1mb.csv')

# Display DataFrame schema
df.printSchema()





# Create a temporary view for querying
df.createOrReplaceTempView("temp_customers")

# Query the view
spark.sql("SELECT * FROM temp_customers LIMIT 5").show()





# Creating a Managed Table
spark.sql("DROP TABLE IF EXISTS managed_customers")
spark.sql("""
    CREATE TABLE managed_customers AS 
    SELECT * FROM temp_customers
""")
print("✅ Managed table 'managed_customers' created.")


spark.sql("ALTER DATABASE default SET LOCATION 'file:/tmp/mera/warehouse'")



spark.sql('describe extended managed_customers').show(truncate=False)








# Creating an External Table
spark.sql("DROP TABLE IF EXISTS external_customers")
spark.sql("""
    CREATE EXTERNAL TABLE external_customers 
    USING CSV 
    LOCATION '/tmp/customers_1mb.csv'
""")
print("✅ External table 'external_customers' created.")








# Show tables in Spark
spark.sql("SHOW TABLES").show()





# Drop managed table (Data is deleted!)
spark.sql("DROP TABLE IF EXISTS managed_customers")

# Drop external table (Data is NOT deleted!)
spark.sql("DROP TABLE IF EXISTS external_customers")

print("✅ Tables dropped successfully.")



