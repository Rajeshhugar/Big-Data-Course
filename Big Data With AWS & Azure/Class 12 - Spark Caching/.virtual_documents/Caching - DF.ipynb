from pyspark.sql import SparkSession


spark.stop()


spark = SparkSession.builder \
.appName('DataFrame_caching_demo')\
.enableHiveSupport()\
.getOrCreate()


customers_df= spark.read.option('header','true').csv('/tmp/customers_500mb.csv')


customers_df.printSchema()


customers_df.count()


customers_df.cache() # ->customers_df.cache().show()


customers_df.count()





customers_df.show(5)


customers_df.show(5)


tail_df.unpersist()


customers_df.unpersist()


tail_df = customers_df.orderBy('customer_id',ascending=False)


tail_df.show(5)


tail_df.cache()


tail_df.show(5)


tail_df.show(5)


spark.stop()
