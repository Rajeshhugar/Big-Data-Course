{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e74720b-36c3-4d6b-96f5-c46b851c9126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 03:09:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|customer_id|      name|     city|      state|country|registration_date|is_active|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n",
      "|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n",
      "|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n",
      "|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n",
      "|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TableDemo\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV from HDFS\n",
    "hdfs_path = \"/tmp/customers_100.csv\"  # Update path as per your HDFS location\n",
    "df = spark.read.option(\"header\", True).csv(hdfs_path)\n",
    "\n",
    "# Show Data\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49e3be9a-d909-4b6b-bb55-754133c023c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|tableName     |isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|         |temp_customers|true       |\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd376f4-3cbd-4cd2-9763-4cb48d862deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('drop table temp_customers_2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d1d874-eb52-43a8-b315-0262df214f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Querying Temporary Table (Exists only in this session) ###\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|customer_id|      name|     city|      state|country|registration_date|is_active|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n",
      "|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n",
      "|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n",
      "|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n",
      "|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Step 1: Create a Temporary Table (Session-based) ---------------- #\n",
    "df.createOrReplaceTempView(\"temp_customers\")\n",
    "\n",
    "print(\"### Querying Temporary Table (Exists only in this session) ###\")\n",
    "spark.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00ce743e-467f-4f77-a21b-61c932bbc97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Querying Global Temporary Table ###\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|customer_id|      name|     city|      state|country|registration_date|is_active|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n",
      "|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n",
      "|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n",
      "|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n",
      "|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Step 2: Create a Global Temporary Table (Accessible across sessions) ---------------- #\n",
    "df.createOrReplaceGlobalTempView(\"global_customers\")\n",
    "\n",
    "print(\"### Querying Global Temporary Table ###\")\n",
    "spark.sql(\"SELECT * FROM global_temp.global_customers LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ce38b4e-4cce-41e9-ad83-cace9ac6db05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|customer_id|      name|     city|      state|country|registration_date|is_active|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n",
      "|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n",
      "|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n",
      "|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n",
      "|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from global_temp.global_customers limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8efacdf3-c44f-4169-b6d0-c14559c26419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 03:16:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Querying Persistent Table (Accessible across sessions and applications) ###\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|customer_id|      name|     city|      state|country|registration_date|is_active|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n",
      "|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n",
      "|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n",
      "|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n",
      "|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n",
      "+-----------+----------+---------+-----------+-------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- Step 3: Create a Persistent Table (Stored in Hive Metastore) ---------------- #\n",
    "spark.sql(\"DROP TABLE IF EXISTS customers_persistent\")\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"customers_persistent\")\n",
    "\n",
    "print(\"### Querying Persistent Table (Accessible across sessions and applications) ###\")\n",
    "spark.sql(\"SELECT * FROM customers_persistent LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b378096c-4ca1-4f17-b97f-628a07ac46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                   |comment|\n",
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "|customer_id                 |string                                                      |null   |\n",
      "|name                        |string                                                      |null   |\n",
      "|city                        |string                                                      |null   |\n",
      "|state                       |string                                                      |null   |\n",
      "|country                     |string                                                      |null   |\n",
      "|registration_date           |string                                                      |null   |\n",
      "|is_active                   |string                                                      |null   |\n",
      "|                            |                                                            |       |\n",
      "|# Detailed Table Information|                                                            |       |\n",
      "|Database                    |default                                                     |       |\n",
      "|Table                       |customers_persistent                                        |       |\n",
      "|Owner                       |root                                                        |       |\n",
      "|Created Time                |Sun Feb 02 03:16:06 UTC 2025                                |       |\n",
      "|Last Access                 |UNKNOWN                                                     |       |\n",
      "|Created By                  |Spark 3.3.2                                                 |       |\n",
      "|Type                        |MANAGED                                                     |       |\n",
      "|Provider                    |parquet                                                     |       |\n",
      "|Statistics                  |3898 bytes                                                  |       |\n",
      "|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers_persistent|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe |       |\n",
      "+----------------------------+------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended customers_persistent').show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c705311b-8554-4e0b-900c-06f9669603e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Querying Global Temporary Table ###\n",
      "+-----------+----------------+-----------+\n",
      "|  namespace|       tableName|isTemporary|\n",
      "+-----------+----------------+-----------+\n",
      "|global_temp|global_customers|       true|\n",
      "+-----------+----------------+-----------+\n",
      "\n",
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|customer_id|       name|     city|      state|country|registration_date|is_active|\n",
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n",
      "|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n",
      "|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n",
      "|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n",
      "|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n",
      "|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    False|\n",
      "|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    False|\n",
      "|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     True|\n",
      "|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     True|\n",
      "|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     True|\n",
      "|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     True|\n",
      "|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    False|\n",
      "|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    False|\n",
      "|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     True|\n",
      "|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    False|\n",
      "|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     True|\n",
      "|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    False|\n",
      "|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    False|\n",
      "|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     True|\n",
      "|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     True|\n",
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new session within the same Spark application\n",
    "spark_new = spark.newSession()\n",
    "\n",
    "# Verify the view exists\n",
    "print(\"### Querying Global Temporary Table ###\")\n",
    "spark_new.sql(\"SHOW TABLES IN global_temp\").show()\n",
    "\n",
    "# Query the Global Temp View\n",
    "spark_new.sql(\"SELECT * FROM global_temp.global_customers\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c1ea689-ac2e-4278-ba10-460f3109e1aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: temp_customers; line 1 pos 14;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project [*]\n      +- 'UnresolvedRelation [temp_customers], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9027/193810830.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM temp_customers LIMIT 5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: temp_customers; line 1 pos 14;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project [*]\n      +- 'UnresolvedRelation [temp_customers], [], false\n"
     ]
    }
   ],
   "source": [
    "spark_new.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "593764bf-20f1-4509-ad71-c0d76d8107a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|customers_persistent|      false|\n",
      "|         |      temp_customers|       true|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f2862a2-ee6c-49f9-aa9c-cc727aa8b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
