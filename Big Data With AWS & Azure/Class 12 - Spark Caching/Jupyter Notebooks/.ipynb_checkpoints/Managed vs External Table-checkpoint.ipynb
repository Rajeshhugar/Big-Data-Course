{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Managed vs External Tables in Spark\n",
    "\n",
    "This notebook explains **Managed vs External Tables in Spark** using an example dataset (`customers_1mb.csv`). It also demonstrates how to configure the **default warehouse path** and verify tables using **Hive Metastore**.\n",
    "\n",
    "## üîπ Key Concepts\n",
    "- **Managed Table**: Spark fully controls the **data and metadata**.\n",
    "- **External Table**: Spark manages only **metadata**, while the data remains **outside** Spark's control.\n",
    "- **Hive Metastore**: Stores **table definitions** to enable SQL-like querying in Spark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 03:33:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 1: Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# ‚úÖ Step 3: Create Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('ManagedVsExternalTables') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# print(f\"Spark session created with warehouse location: {warehouse_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Checking Current Warehouse Directory\n",
    "You can verify the current **Spark SQL warehouse directory** using the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/spark-warehouse'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the configured warehouse directory\n",
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Loading the Dataset\n",
    "Now, we load the **customers_1mb.csv** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- registration_date: timestamp (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load CSV data into a DataFrame\n",
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header', 'True') \\\n",
    "    .option('inferSchema', 'True') \\\n",
    "    .load('/tmp/customers_100.csv')\n",
    "\n",
    "# Display DataFrame schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Creating a Temporary View\n",
    "We'll create a **temporary view** to allow SQL-like queries before creating tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "|customer_id|      name|     city|      state|country|  registration_date|is_active|\n",
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n",
      "|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n",
      "|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n",
      "|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n",
      "|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n",
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view for querying\n",
    "df.createOrReplaceTempView(\"temp_customers\")\n",
    "\n",
    "# Query the view\n",
    "spark.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèó Creating a **Managed Table**\n",
    "- Spark **stores** the data inside the **warehouse directory** (`/tmp/mera/warehouse`).\n",
    "- If you **drop** this table, the **data is also deleted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "25/02/02 03:35:50 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "25/02/02 03:35:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Managed table 'managed_customers' created.\n"
     ]
    }
   ],
   "source": [
    "# Creating a Managed Table\n",
    "spark.sql(\"DROP TABLE IF EXISTS managed_customers\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE managed_customers AS \n",
    "    SELECT * FROM temp_customers\n",
    "\"\"\")\n",
    "print(\"‚úÖ Managed table 'managed_customers' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                |comment|\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|customer_id                 |int                                                      |null   |\n",
      "|name                        |string                                                   |null   |\n",
      "|city                        |string                                                   |null   |\n",
      "|state                       |string                                                   |null   |\n",
      "|country                     |string                                                   |null   |\n",
      "|registration_date           |timestamp                                                |null   |\n",
      "|is_active                   |boolean                                                  |null   |\n",
      "|                            |                                                         |       |\n",
      "|# Detailed Table Information|                                                         |       |\n",
      "|Database                    |default                                                  |       |\n",
      "|Table                       |managed_customers                                        |       |\n",
      "|Owner                       |root                                                     |       |\n",
      "|Created Time                |Sun Feb 02 03:35:50 UTC 2025                             |       |\n",
      "|Last Access                 |UNKNOWN                                                  |       |\n",
      "|Created By                  |Spark 3.3.2                                              |       |\n",
      "|Type                        |MANAGED                                                  |       |\n",
      "|Provider                    |hive                                                     |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1738467352]                       |       |\n",
      "|Statistics                  |6315 bytes                                               |       |\n",
      "|Location                    |hdfs://my-cluster-m/user/hive/warehouse/managed_customers|       |\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended managed_customers').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxr-xr-x   2 root hadoop       6315 2025-02-02 03:35 /user/hive/warehouse/managed_customers/part-00000-695ee9bd-dbce-4dda-8f90-c66e35b19d20-c000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/managed_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Creating an **External Table**\n",
    "- The data **remains** in `/tmp/customers_1mb.csv`.\n",
    "- If you **drop** this table, the **data is not deleted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ External table 'external_customers' created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/02 03:41:16 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider CSV. Persisting data source table `default`.`external_customers` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    }
   ],
   "source": [
    "# Creating an External Table\n",
    "spark.sql(\"DROP TABLE IF EXISTS external_customers\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE external_customers \n",
    "    USING CSV \n",
    "    LOCATION '/tmp/customers_1mb.csv'\n",
    "\"\"\")\n",
    "print(\"‚úÖ External table 'external_customers' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                         |comment|\n",
      "+----------------------------+--------------------------------------------------+-------+\n",
      "|_c0                         |string                                            |null   |\n",
      "|_c1                         |string                                            |null   |\n",
      "|_c2                         |string                                            |null   |\n",
      "|_c3                         |string                                            |null   |\n",
      "|_c4                         |string                                            |null   |\n",
      "|_c5                         |string                                            |null   |\n",
      "|_c6                         |string                                            |null   |\n",
      "|                            |                                                  |       |\n",
      "|# Detailed Table Information|                                                  |       |\n",
      "|Database                    |default                                           |       |\n",
      "|Table                       |external_customers                                |       |\n",
      "|Owner                       |root                                              |       |\n",
      "|Created Time                |Sun Feb 02 03:41:16 UTC 2025                      |       |\n",
      "|Last Access                 |UNKNOWN                                           |       |\n",
      "|Created By                  |Spark 3.3.2                                       |       |\n",
      "|Type                        |EXTERNAL                                          |       |\n",
      "|Provider                    |CSV                                               |       |\n",
      "|Location                    |hdfs://my-cluster-m/tmp/customers_1mb.csv         |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat  |       |\n",
      "+----------------------------+--------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended external_customers').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Verify External Table\n",
    "Run the following command to check its location:\n",
    "```bash\n",
    "!hdfs dfs -ls /tmp/customers_1mb.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Verifying Tables in Hive Metastore\n",
    "You can check all available tables in Spark using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|customers_persistent|      false|\n",
      "|  default|  external_customers|      false|\n",
      "|  default|   managed_customers|      false|\n",
      "|         |      temp_customers|       true|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in Spark\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõë Dropping the Tables\n",
    "Dropping a **Managed Table** deletes both **metadata and data**, while dropping an **External Table** deletes only **metadata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tables dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Drop managed table (Data is deleted!)\n",
    "spark.sql(\"DROP TABLE IF EXISTS managed_customers\")\n",
    "\n",
    "# Drop external table (Data is NOT deleted!)\n",
    "spark.sql(\"DROP TABLE IF EXISTS external_customers\")\n",
    "\n",
    "print(\" Tables dropped successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summary\n",
    "| Feature | Managed Table | External Table |\n",
    "|---------|--------------|----------------|\n",
    "| Data Location | Inside warehouse | Custom location |\n",
    "| Dropping Table | Deletes data | Only deletes metadata |\n",
    "| Performance | Optimized by Spark | Depends on external storage |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Spark uses Hive Metastore for Sceh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir /tmp/mera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/mera/warehouse': File exists\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir /tmp/mera/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21247/3499198947.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ‚úÖ Step 3: Create Spark session with Hive support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mspark_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.sql.warehouse.dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarehouse_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0menableHiveSupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mnewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 1: Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "warehouse_location = '/tmp/mera/warehouse'\n",
    "\n",
    "# ‚úÖ Step 3: Create Spark session with Hive support\n",
    "spark_self = spark \\\n",
    "    .config('spark.sql.warehouse.dir', warehouse_location)\\\n",
    "    .enableHiveSupport() \\\n",
    "    .newSession()\n",
    "\n",
    "# print(f\"Spark session created with warehouse location: {warehouse_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/spark-warehouse'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the configured warehouse directory\n",
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
