from pyspark.sql import SparkSession


spark = SparkSession.builder \
.appName("Spark_Table") \
.getOrCreate()


spark


df = spark.read \
.format('csv')\
.option('header','True')\
.option('inferSchema','True')\
.load('/tmp/customers_1mb.csv')


spark.sql('show databases').show()


df.write.format('csv').saveAsTable('default.customers_2')


spark.sql('describe extended customers_2').show(truncate=False)


df_2 = spark.sql('select * from customers_2 limit 5')


df_2.show()











df = spark.read \
.format('csv')\
.option('header','True')\
.option('inferSchema','True')\
.load('/tmp/customers_500mb.csv')


df_2 = spark.sql('select * from customers_2 limit 5')





df_3 = spark.table('customers_2')


df_4 = spark.range(0,10)
df_4.show()



data = [(1, "Alice", "Mumbai", "2023-01-15", True),
(2, "Bob", "Delhi", "2023-03-25", False),
(3, "Charlie", "Chennai", "2023-05-10", True)]

columns = ["customer_id", "name", "city", "registration_date", "is_active"]
df_5 = spark.createDataFrame(data,columns)


df_5.show()


rdd=spark.sparkContext.parallelize(data)


df_6 = rdd.toDF(["customer_id", "name", "city", "registration_date", "is_active"])


df_7 = spark.createDataFrame(rdd).toDF("customer_id", "name", "city", "registration_date", "is_active")


df_6.show()


data = [ 
{
    "total": 3,
    "subtotal": 3,
    "page": 1,
    "per_page": 20,
    "search": null,
    "sort": {
        "by": null,
        "order": null
    },
    {
        "by": null,
        "order": null
    },
    {
        "by": null,
        "order": null
    },
    "results": [
        {
            "id": 23,
            "name": "qa.lab.example.com",
            "fullname": "QA",
            "dns_id": 10,
            "created_at": "2013-08-13T09:02:31Z",
            "updated_at": "2013-08-13T09:02:31Z"
        },
        {
            "id": 25,
            "name": "sat.lab.example.com",
            "fullname": "SATLAB",
            "dns_id": 8,
            "created_at": "2013-08-13T08:32:48Z",
            "updated_at": "2013-08-14T07:04:03Z"
        },
        {
            "id": 32,
            "name": "hr.lab.example.com",
            "fullname": "HR",
            "dns_id": 8,
            "created_at": "2013-08-16T08:32:48Z",
            "updated_at": "2013-08-16T07:04:03Z"
        }
    ]
}
]


from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType


schema_nested = StructType([
    StructField("total", IntegerType(), nullable=False),
    StructField("subtotal", IntegerType(), nullable=False),
    StructField("page", IntegerType(), nullable=False),
    StructField("per_page", IntegerType(), nullable=False),
    StructField("search", StringType(), nullable=True),
    StructField("sort", StructType([
        StructField("by", StringType(), nullable=True),
        StructField("order", StringType(), nullable=True)
    ]), nullable=True),
    StructField("results", ArrayType(StructType([
        StructField("id", IntegerType(), nullable=False),
        StructField("name", StringType(), nullable=True),
        StructField("fullname", StringType(), nullable=True),
        StructField("dns_id", IntegerType(), nullable=False),
        StructField("created_at", StringType(), nullable=True),  # Timestamp as string
        StructField("updated_at", StringType(), nullable=True)   # Timestamp as string
    ])), nullable=False)
])



neted_df = spark.read.format('json').schema(schema_nested).option('multiline','True').load('/tmp/nested_structure.json')


neted_df.describe()





spark.stop()
