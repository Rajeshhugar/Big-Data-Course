{"cells": [{"cell_type": "code", "execution_count": 2, "id": "bd1339bf-3d11-42d0-81cd-ea765d6ee9c1", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 1, "id": "5eb40875-f019-41d6-9eee-3c1655fcbf01", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/01 04:55:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n.appName(\"Spark_Table\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "34f9cd77-6af6-4305-b4db-d61b3aa5649b", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-cluster-m.us-central1-c.c.bigdata-project-448115.internal:44401\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fa482842d70>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 3, "id": "c6505e62-b048-4e54-a67f-4f0f7900e6d3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read \\\n.format('csv')\\\n.option('header','True')\\\n.option('inferSchema','True')\\\n.load('/tmp/customers_1mb.csv')"}, {"cell_type": "code", "execution_count": 6, "id": "13eb94b3-7a66-4d47-a393-8898b81872aa", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+\n|namespace|\n+---------+\n|  default|\n+---------+\n\n"}], "source": "spark.sql('show databases').show()"}, {"cell_type": "code", "execution_count": 8, "id": "726cc1ac-2517-40c1-ac7b-88a9252835f5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/01 04:57:05 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `default`.`customers_2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n25/02/01 04:57:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}], "source": "df.write.format('csv').saveAsTable('default.customers_2')"}, {"cell_type": "code", "execution_count": 9, "id": "1a91858d-1196-496f-99b7-ac353a8d7c7e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+---------------------------------------------------+-------+\n|col_name                    |data_type                                          |comment|\n+----------------------------+---------------------------------------------------+-------+\n|customer_id                 |int                                                |null   |\n|name                        |string                                             |null   |\n|city                        |string                                             |null   |\n|state                       |string                                             |null   |\n|country                     |string                                             |null   |\n|registration_date           |timestamp                                          |null   |\n|is_active                   |boolean                                            |null   |\n|                            |                                                   |       |\n|# Detailed Table Information|                                                   |       |\n|Database                    |default                                            |       |\n|Table                       |customers_2                                        |       |\n|Owner                       |root                                               |       |\n|Created Time                |Sat Feb 01 04:57:06 UTC 2025                       |       |\n|Last Access                 |UNKNOWN                                            |       |\n|Created By                  |Spark 3.3.2                                        |       |\n|Type                        |MANAGED                                            |       |\n|Provider                    |csv                                                |       |\n|Statistics                  |1307828 bytes                                      |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers_2|       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |       |\n+----------------------------+---------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended customers_2').show(truncate=False)"}, {"cell_type": "code", "execution_count": 12, "id": "a2d9e746-87a5-4de2-95a8-ab6e1868226b", "metadata": {}, "outputs": [], "source": "df_2 = spark.sql('select * from customers_2 limit 5')"}, {"cell_type": "code", "execution_count": 13, "id": "9a991a75-51c8-434c-b892-5099882ab2bb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-------------------+---------+\n|customer_id|      name|     city|      state|country|  registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n\n"}], "source": "df_2.show()"}, {"cell_type": "code", "execution_count": null, "id": "9595688e-8b47-4d08-bd8e-281a62f2a00c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "302a97ec-0f0a-4f26-a114-c49dc44dbd49", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "09fb0a7b-15c7-4d9e-b17e-6c9e5692a4ff", "metadata": {}, "source": "# How to create Spark DF"}, {"cell_type": "code", "execution_count": 45, "id": "cf360838-933c-430a-8bea-cd38622b2c3f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read \\\n.format('csv')\\\n.option('header','True')\\\n.option('inferSchema','True')\\\n.load('/tmp/customers_500mb.csv')"}, {"cell_type": "code", "execution_count": null, "id": "f90ec9f9-a9f3-4eca-983a-3ec2c3c3396d", "metadata": {}, "outputs": [], "source": "df_2 = spark.sql('select * from customers_2 limit 5')"}, {"cell_type": "code", "execution_count": null, "id": "26dd91b4-884e-4aac-9795-0b0fcfda5d10", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 14, "id": "83a667be-6b1f-41cd-9ea2-4c2e1d93cd4e", "metadata": {}, "outputs": [], "source": "df_3 = spark.table('customers_2')"}, {"cell_type": "code", "execution_count": 15, "id": "39b1b341-fbc6-413e-a728-5834aaf2bf48", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n+---+\n\n"}], "source": "df_4 = spark.range(0,10)\ndf_4.show()"}, {"cell_type": "code", "execution_count": 16, "id": "51dcd6ce-319d-4c9a-8ddc-a4b1cdbaf614", "metadata": {}, "outputs": [], "source": "\ndata = [(1, \"Alice\", \"Mumbai\", \"2023-01-15\", True),\n(2, \"Bob\", \"Delhi\", \"2023-03-25\", False),\n(3, \"Charlie\", \"Chennai\", \"2023-05-10\", True)]\n\ncolumns = [\"customer_id\", \"name\", \"city\", \"registration_date\", \"is_active\"]\ndf_5 = spark.createDataFrame(data,columns)"}, {"cell_type": "code", "execution_count": 17, "id": "6b222028-4dbf-4e6a-9739-e91a13d42b2f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 10:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------+-----------------+---------+\n|customer_id|   name|   city|registration_date|is_active|\n+-----------+-------+-------+-----------------+---------+\n|          1|  Alice| Mumbai|       2023-01-15|     true|\n|          2|    Bob|  Delhi|       2023-03-25|    false|\n|          3|Charlie|Chennai|       2023-05-10|     true|\n+-----------+-------+-------+-----------------+---------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_5.show()"}, {"cell_type": "code", "execution_count": 18, "id": "7f8cd97f-9214-4762-aac7-60b361eb4c85", "metadata": {}, "outputs": [], "source": "rdd=spark.sparkContext.parallelize(data)"}, {"cell_type": "code", "execution_count": 20, "id": "5dcac5c1-283e-4d50-8b0e-12e99daf5771", "metadata": {}, "outputs": [], "source": "df_6 = rdd.toDF([\"customer_id\", \"name\", \"city\", \"registration_date\", \"is_active\"])"}, {"cell_type": "code", "execution_count": 25, "id": "935d2356-c46d-45f4-a183-2b70078fb013", "metadata": {}, "outputs": [], "source": "df_7 = spark.createDataFrame(rdd).toDF(\"customer_id\", \"name\", \"city\", \"registration_date\", \"is_active\")"}, {"cell_type": "code", "execution_count": 21, "id": "f410af60-b9b3-419f-a4c4-f991b5d175ac", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------+-----------------+---------+\n|customer_id|   name|   city|registration_date|is_active|\n+-----------+-------+-------+-----------------+---------+\n|          1|  Alice| Mumbai|       2023-01-15|     true|\n|          2|    Bob|  Delhi|       2023-03-25|    false|\n|          3|Charlie|Chennai|       2023-05-10|     true|\n+-----------+-------+-------+-----------------+---------+\n\n"}], "source": "df_6.show()"}, {"cell_type": "code", "execution_count": 28, "id": "c566e945-f450-4e2f-93c8-110c2e76299a", "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'null' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_12098/2273518087.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"page\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"per_page\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m\"search\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnull\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \"sort\": {\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"by\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnull\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'null' is not defined"]}], "source": "data = [ \n{\n    \"total\": 3,\n    \"subtotal\": 3,\n    \"page\": 1,\n    \"per_page\": 20,\n    \"search\": null,\n    \"sort\": {\n        \"by\": null,\n        \"order\": null\n    },\n    {\n        \"by\": null,\n        \"order\": null\n    },\n    {\n        \"by\": null,\n        \"order\": null\n    },\n    \"results\": [\n        {\n            \"id\": 23,\n            \"name\": \"qa.lab.example.com\",\n            \"fullname\": \"QA\",\n            \"dns_id\": 10,\n            \"created_at\": \"2013-08-13T09:02:31Z\",\n            \"updated_at\": \"2013-08-13T09:02:31Z\"\n        },\n        {\n            \"id\": 25,\n            \"name\": \"sat.lab.example.com\",\n            \"fullname\": \"SATLAB\",\n            \"dns_id\": 8,\n            \"created_at\": \"2013-08-13T08:32:48Z\",\n            \"updated_at\": \"2013-08-14T07:04:03Z\"\n        },\n        {\n            \"id\": 32,\n            \"name\": \"hr.lab.example.com\",\n            \"fullname\": \"HR\",\n            \"dns_id\": 8,\n            \"created_at\": \"2013-08-16T08:32:48Z\",\n            \"updated_at\": \"2013-08-16T07:04:03Z\"\n        }\n    ]\n}\n]"}, {"cell_type": "code", "execution_count": 35, "id": "c62a0636-76e8-4c48-8754-49acf512b96a", "metadata": {}, "outputs": [], "source": "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n\n\nschema_nested = StructType([\n    StructField(\"total\", IntegerType(), nullable=False),\n    StructField(\"subtotal\", IntegerType(), nullable=False),\n    StructField(\"page\", IntegerType(), nullable=False),\n    StructField(\"per_page\", IntegerType(), nullable=False),\n    StructField(\"search\", StringType(), nullable=True),\n    StructField(\"sort\", StructType([\n        StructField(\"by\", StringType(), nullable=True),\n        StructField(\"order\", StringType(), nullable=True)\n    ]), nullable=True),\n    StructField(\"results\", ArrayType(StructType([\n        StructField(\"id\", IntegerType(), nullable=False),\n        StructField(\"name\", StringType(), nullable=True),\n        StructField(\"fullname\", StringType(), nullable=True),\n        StructField(\"dns_id\", IntegerType(), nullable=False),\n        StructField(\"created_at\", StringType(), nullable=True),  # Timestamp as string\n        StructField(\"updated_at\", StringType(), nullable=True)   # Timestamp as string\n    ])), nullable=False)\n])\n"}, {"cell_type": "code", "execution_count": 38, "id": "2479ae0c-9e19-420a-ab6d-f1ba8ee02f6a", "metadata": {}, "outputs": [], "source": "neted_df = spark.read.format('json').schema(schema_nested).option('multiline','True').load('/tmp/nested_structure.json')"}, {"cell_type": "code", "execution_count": 44, "id": "096e2247-386c-4f4f-a579-10d38cfca14a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "DataFrame[summary: string, total: string, subtotal: string, page: string, per_page: string, search: string]"}, "execution_count": 44, "metadata": {}, "output_type": "execute_result"}], "source": "neted_df.describe()"}, {"cell_type": "code", "execution_count": null, "id": "28ee8975-58e4-4194-81ea-e42a376e65df", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 49, "id": "f36fe962-2bfd-4761-a504-f48721143c70", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}