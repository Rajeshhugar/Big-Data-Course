{"cells": [{"cell_type": "code", "execution_count": null, "id": "f190e751-a41f-49b9-9230-fd84aa2c2ad4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/01 03:52:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n25/02/01 03:52:21 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n25/02/01 03:52:36 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n25/02/01 03:52:51 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n25/02/01 03:53:06 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"}], "source": "from pyspark.sql import SparkSession\n\n# Initialize SparkSession with Hive support\nspark = SparkSession.builder \\\n    .appName(\"TableDemo\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Read CSV from HDFS\nhdfs_path = \"/tmp/customers_1mb.csv\"  # Update path as per your HDFS location\ndf = spark.read.option(\"header\", True).csv(hdfs_path)\n\n# Show Data\ndf.show(5)\n\n# ---------------- Step 1: Create a Temporary Table (Session-based) ---------------- #\ndf.createOrReplaceTempView(\"temp_customers\")\n\nprint(\"### Querying Temporary Table (Exists only in this session) ###\")\nspark.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()\n\n# ---------------- Step 2: Create a Global Temporary Table (Accessible across sessions) ---------------- #\ndf.createOrReplaceGlobalTempView(\"global_customers\")\n\nprint(\"### Querying Global Temporary Table ###\")\nspark.sql(\"SELECT * FROM global_temp.global_customers LIMIT 5\").show()\n\n# ---------------- Step 3: Create a Persistent Table (Stored in Hive Metastore) ---------------- #\nspark.sql(\"DROP TABLE IF EXISTS customers_persistent\")\ndf.write.mode(\"overwrite\").saveAsTable(\"customers_persistent\")\n\nprint(\"### Querying Persistent Table (Accessible across sessions and applications) ###\")\nspark.sql(\"SELECT * FROM customers_persistent LIMIT 5\").show()\n"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}