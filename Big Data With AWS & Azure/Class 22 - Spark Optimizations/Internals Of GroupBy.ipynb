{"metadata": {"kernelspec": {"name": "python", "display_name": "Python (Pyodide)", "language": "python"}, "language_info": {"codemirror_mode": {"name": "python", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Internals of groupBy in PySpark\n\n## Table of Contents\n1. [Introduction to groupBy](#1-introduction-to-groupby)\n2. [How groupBy Works Internally](#2-how-groupby-works-internally)\n3. [Shuffling and Partitioning in groupBy](#3-shuffling-and-partitioning-in-groupby)\n4. [Impact of Adaptive Query Execution (AQE)](#4-impact-of-adaptive-query-execution-aqe)\n5. [Optimizing groupBy Operations](#5-optimizing-groupby-operations)\n6. [Properties and Configurations](#6-properties-and-configurations)\n7. [Summary](#7-summary)", "metadata": {}}, {"cell_type": "markdown", "source": "## 1. Introduction to groupBy\n\nThe `groupBy` operation in PySpark is a **wide transformation** that groups rows in a DataFrame based on one or more columns. It is commonly used for aggregations, such as counting, summing, or averaging values within each group.\n\n### Why is groupBy Important?\n- **Data Aggregation**: Allows you to summarize data by grouping it based on specific columns.\n- **Data Analysis**: Useful for analyzing trends and patterns within subsets of data.\n- **Performance Considerations**: Understanding how `groupBy` works internally helps optimize performance, especially for large datasets.\n\n### Key Concepts\n- **Wide Transformation**: `groupBy` is a wide transformation because it requires shuffling data across the cluster.\n- **Shuffling**: The process of redistributing data across the cluster based on the group key.\n- **Partitions**: Data is divided into partitions, and shuffling ensures that rows with the same key are processed together.", "metadata": {}}, {"cell_type": "markdown", "source": "## 2. How groupBy Works Internally\n\nWhen you perform a `groupBy` operation, PySpark follows these steps:\n\n### Diagram: groupBy Process\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  DataFrame    \u2502              \u2502  Shuffled Data \u2502\n\u2502  (Partitioned)\u2502              \u2502  (Grouped)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                              \u2502\n        \u25bc                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Shuffle Data by Group Key       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                               \u2502\n        \u25bc                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Node 1       \u2502              \u2502  Node 2       \u2502\n\u2502  Group 1      \u2502              \u2502  Group 2      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Steps in groupBy\n1. **Local Aggregation**: Each partition performs a local aggregation (e.g., counting) on the group key.\n2. **Shuffling**: Data is shuffled across the network so that rows with the same group key are processed together.\n3. **Global Aggregation**: The shuffled data is aggregated to produce the final result.\n\n### Example: groupBy on Customer Data\n\nLet's assume we have a `customers` DataFrame with 1GB of data, and we want to group by `city` and count the number of customers in each city.\n\n```python\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"groupByExample\").getOrCreate()\n\n# Load customer data (1GB)\ncustomers_df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n\n# Perform groupBy and count\nresult_df = customers_df.groupBy(\"city\").count()\n\n# Show results\nresult_df.show()\n```\n\n**Output:**\n\n| city      | count |\n|-----------|-------|\n| Bangalore | 1000  |\n| Pune      | 800   |\n| Mumbai    | 1200  |\n\n### Key Points:\n- **Local Aggregation**: Each partition performs a local count before shuffling.\n- **Shuffling Overhead**: Shuffling is expensive and can impact performance.\n- **Global Aggregation**: The final result is computed after shuffling.", "metadata": {}}, {"cell_type": "markdown", "source": "## 3. Shuffling and Partitioning in groupBy\n\n### Shuffling in groupBy\nShuffling is the process of redistributing data across the cluster so that rows with the same group key are processed together. By default, PySpark creates **200 shuffle partitions** when a wide transformation like `groupBy` is triggered.\n\n### Partitioning in groupBy\nPartitioning divides the data into smaller chunks (partitions) that can be processed in parallel. However, if the number of unique group keys is small (e.g., 9 unique cities), most of the shuffle partitions will remain empty, leading to inefficiency.\n\n### Example: Shuffle Partitions in groupBy\n\n```python\n# Default shuffle partitions\nprint(\"Default shuffle partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n\n# Perform groupBy and count\nresult_df = customers_df.groupBy(\"city\").count()\n\n# Write results to disk (for testing)\nresult_df.write.format(\"noop\").mode(\"overwrite\").save()\n```\n\n**Output:**\n\n```\nDefault shuffle partitions: 200\n```\n\n### Key Points:\n- **Default Shuffle Partitions**: PySpark creates 200 shuffle partitions by default.\n- **Empty Partitions**: If the number of unique group keys is small, many partitions will remain empty, leading to inefficiency.\n- **Task Scheduler Overhead**: The task scheduler is burdened by empty partitions, as it still needs to create tasks for them.", "metadata": {}}, {"cell_type": "markdown", "source": "## 4. Impact of Adaptive Query Execution (AQE)\n\nAdaptive Query Execution (AQE) is a feature introduced in Spark 3.0 that optimizes query execution at runtime. It dynamically adjusts the number of shuffle partitions, handles partition skew, and switches join strategies based on runtime statistics.\n\n### AQE and groupBy\nWhen AQE is enabled, PySpark dynamically coalesces shuffle partitions to reduce the number of empty partitions. This improves performance by reducing the overhead of the task scheduler.\n\n### Example: Enabling AQE\n\n```python\n# Enable AQE\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n\n# Perform groupBy and count\nresult_df = customers_df.groupBy(\"city\").count()\n\n# Write results to disk (for testing)\nresult_df.write.format(\"noop\").mode(\"overwrite\").save()\n```\n\n### Key Points:\n- **Dynamic Coalescing**: AQE dynamically reduces the number of shuffle partitions based on runtime statistics.\n- **Performance Improvement**: Reduces the overhead of the task scheduler by eliminating empty partitions.\n- **Enabled by Default**: AQE is enabled by default in Spark 3.2 and later.", "metadata": {}}, {"cell_type": "markdown", "source": "## 5. Optimizing groupBy Operations\n\n### Reducing Shuffling\nShuffling is expensive, so reducing the amount of data shuffled can significantly improve performance. Here are some strategies:\n- **Filter Early**: Apply filters before the `groupBy` operation to reduce the amount of data shuffled.\n- **Reduce Columns**: Select only the necessary columns before the `groupBy` operation.\n\n### Example: Filtering Before groupBy\n\n```python\n# Filter data before groupBy\nfiltered_df = customers_df.filter(customers_df[\"city\"] == \"Bangalore\")\n\n# Perform groupBy and count\nresult_df = filtered_df.groupBy(\"city\").count()\n\n# Show results\nresult_df.show()\n```\n\n### Key Points:\n- **Filter Early**: Reduces the amount of data shuffled across the network.\n- **Select Columns**: Select only the necessary columns to reduce the size of the data being shuffled.", "metadata": {}}, {"cell_type": "markdown", "source": "## 6. Properties and Configurations\n\n### PySpark Properties\n- **spark.sql.shuffle.partitions**: Controls the number of shuffle partitions. The default value is 200.\n  ```python\n  spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n  ```\n- **spark.sql.adaptive.enabled**: Enables or disables Adaptive Query Execution (AQE). Set to `true` by default in Spark 3.2 and later.\n  ```python\n  spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n  ```\n\n### Special Considerations\n- **Memory Constraints**: Ensure that each node has enough memory to handle the shuffled data.\n- **Skewed Data**: If the data is skewed, consider using techniques like salting to distribute the data more evenly.", "metadata": {}}, {"cell_type": "markdown", "source": "## 7. Summary\n\nIn this notebook, we explored the **internals of groupBy in PySpark**, including how it works, the impact of shuffling and partitioning, and how to optimize groupBy operations using Adaptive Query Execution (AQE).\n\n### Key Takeaways:\n- **groupBy is a Wide Transformation**: It involves shuffling data across the cluster.\n- **Shuffling Overhead**: Shuffling is expensive, so reducing the amount of data shuffled can improve performance.\n- **Adaptive Query Execution (AQE)**: Dynamically optimizes query execution by reducing shuffle partitions and handling partition skew.\n- **Optimization Strategies**: Filter early, reduce columns, and use AQE to optimize groupBy operations.\n\nBy understanding the internals of groupBy and leveraging optimization techniques, you can significantly improve the performance of your PySpark applications.", "metadata": {}}]}