{"cells": [{"cell_type": "code", "execution_count": null, "id": "23b92645-37ec-4351-ab37-3bc442126d7c", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 1, "id": "ef2f19e9-0b7e-480c-94ae-4e0d3e641bf5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/15 02:59:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder.appName('GroupBy').getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "2a23d33e-015e-4e07-b15b-a092b5216a41", "metadata": {}, "outputs": [], "source": "bucket = 'gs://dataproc-staging-us-central1-458263062208-tw36mmqt'\nfile_path = '/notebooks/jupyter/Data/'"}, {"cell_type": "code", "execution_count": 3, "id": "f1af9302-f9b6-4cfb-948b-e84b0114638b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customers_df = spark.read.csv(bucket+file_path+'customers.csv',header=True,inferSchema=True)"}, {"cell_type": "code", "execution_count": 4, "id": "7ee79943-aeb9-41c6-94ed-5991230d0303", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+-------+-----------+-------+-------------------+---------+\n|customer_id|      name|   city|      state|country|  registration_date|is_active|\n+-----------+----------+-------+-----------+-------+-------------------+---------+\n|          0|Customer_0| Mumbai|  Telangana|  India|2023-03-21 00:00:00|     true|\n|          1|Customer_1|Chennai|West Bengal|  India|2023-05-27 00:00:00|    false|\n+-----------+----------+-------+-----------+-------+-------------------+---------+\nonly showing top 2 rows\n\n"}], "source": "customers_df.show(2)"}, {"cell_type": "code", "execution_count": 5, "id": "65ad4891-cea7-4363-bf36-9103e49e0ee0", "metadata": {}, "outputs": [{"data": {"text/plain": "5"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "customers_df.rdd.getNumPartitions()"}, {"cell_type": "code", "execution_count": 8, "id": "413104b6-e3af-46d1-8826-4b9afa58ec72", "metadata": {}, "outputs": [], "source": "results_df = customers_df.groupBy('city').count()"}, {"cell_type": "code", "execution_count": 9, "id": "bf757d63-abb8-4753-a67f-8d31002599bf", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 3:===============================================>           (4 + 1) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+-------+\n|     city|  count|\n+---------+-------+\n|Bangalore|1094195|\n|  Chennai|1095052|\n|   Mumbai|1095815|\n|Ahmedabad|1097162|\n|  Kolkata|1096777|\n|     Pune|1095748|\n|    Delhi|1096183|\n|Hyderabad|1096426|\n+---------+-------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "results_df.show()"}, {"cell_type": "code", "execution_count": 10, "id": "4b941eff-4dac-41e0-93e5-5d0b1af2b342", "metadata": {}, "outputs": [], "source": "spark.conf.set('spark.sql.adaptive.enabled','false')"}, {"cell_type": "code", "execution_count": 12, "id": "a03cfa8e-bed7-4720-8597-4311b5d8771e", "metadata": {}, "outputs": [{"data": {"text/plain": "'200'"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "spark.conf.get('spark.sql.shuffle.partitions')"}, {"cell_type": "code", "execution_count": 11, "id": "414179d7-1c27-4535-9bff-a6bbada8d416", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customers_df.groupBy('city').count().write.format('parquet').mode('overwrite').save('/data')"}, {"cell_type": "markdown", "id": "a8939ac7-d0b9-489d-92a3-b116e9d51719", "metadata": {}, "source": "hash functions would be used as part of bucketing step  , what if we don't do bucketing how will it work"}, {"cell_type": "markdown", "id": "1095ae2e-6f0b-4612-9afa-d4c1f4467663", "metadata": {}, "source": "But it is still not optimized to count them without moving them"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}