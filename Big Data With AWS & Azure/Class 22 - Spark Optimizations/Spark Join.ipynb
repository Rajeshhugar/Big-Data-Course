{"cells": [{"cell_type": "code", "execution_count": 1, "id": "4116f694-a960-43e4-8662-02e66c051743", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "7f993946-746e-4cf0-9497-ad9db0466bb0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/15 03:53:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder.appName('Join').getOrCreate()"}, {"cell_type": "code", "execution_count": 4, "id": "28a1f091-5f99-44e1-9ed3-60463b2f3f9d", "metadata": {}, "outputs": [{"data": {"text/plain": "'application_1742006069642_0005'"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.applicationId"}, {"cell_type": "code", "execution_count": 5, "id": "e141603f-a89d-4ae0-a245-2c9f94351583", "metadata": {}, "outputs": [], "source": "spark.conf.set('spark.sql.adaptive.enabled','false')"}, {"cell_type": "code", "execution_count": 6, "id": "6f9540a6-28d1-462a-b4ff-beb2ebc82224", "metadata": {}, "outputs": [], "source": "bucket = 'gs://dataproc-staging-us-central1-458263062208-tw36mmqt'\nfile_path = '/notebooks/jupyter/Data/'"}, {"cell_type": "code", "execution_count": 7, "id": "f00898ae-6158-481b-b418-511ae4214bb9", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customers_df = spark.read.csv(bucket+file_path+'customers.csv',header=True,inferSchema=True)\norders_df = spark.read.csv(bucket+file_path+'orders.csv',header=True,inferSchema=True)"}, {"cell_type": "code", "execution_count": 8, "id": "25ec09e7-9bd1-4114-8f11-cebe37aea3e4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+-------+-----------+-------+-------------------+---------+\n|customer_id|      name|   city|      state|country|  registration_date|is_active|\n+-----------+----------+-------+-----------+-------+-------------------+---------+\n|          0|Customer_0| Mumbai|  Telangana|  India|2023-03-21 00:00:00|     true|\n|          1|Customer_1|Chennai|West Bengal|  India|2023-05-27 00:00:00|    false|\n+-----------+----------+-------+-----------+-------+-------------------+---------+\nonly showing top 2 rows\n\n+--------+-----------+-------------------+-----------------+-------+\n|order_id|customer_id|         order_date|     total_amount| status|\n+--------+-----------+-------------------+-----------------+-------+\n|       0|    3194509|2024-04-12 00:00:00|772.3507972244216|Shipped|\n|       1|    8003925|2024-05-06 00:00:00| 404.843827081314|Shipped|\n+--------+-----------+-------------------+-----------------+-------+\nonly showing top 2 rows\n\n"}, {"data": {"text/plain": "(None, None)"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "customers_df.show(2),orders_df.show(2)"}, {"cell_type": "code", "execution_count": 9, "id": "597644fa-6023-43ff-913a-8753cb3020b3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\n-rw-r--r--   2 tech_mayankagg hadoop      1.0 M 2025-03-01 03:34 /data2/customers.csv\n"}], "source": "!hadoop fs -ls -h /data2/"}, {"cell_type": "code", "execution_count": 10, "id": "4659eb1b-8b2a-4e93-a752-ab6be2d32b90", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customers_small_df = spark.read.csv('/data2/customers.csv',header=True,inferSchema=True)"}, {"cell_type": "code", "execution_count": 11, "id": "6f4b9653-2435-4b9a-8540-02411d7c3c3a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 8:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-------------------+---------+\n|customer_id|      name|     city|      state|country|  registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\nonly showing top 2 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customers_small_df.show(2)"}, {"cell_type": "code", "execution_count": 12, "id": "04393569-52a8-475e-9a7e-edba9d317002", "metadata": {}, "outputs": [], "source": "joined_df = orders_df.join(customers_small_df,on='customer_id',how='inner')"}, {"cell_type": "markdown", "id": "dd498616-b32d-4b8d-afa7-24c00da8aed2", "metadata": {}, "source": "We seem to be working with identical names which is always not the case."}, {"cell_type": "code", "execution_count": 13, "id": "cf8e5819-d46c-4e70-a99e-16dbe3bb8e55", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Don't do show() as that might take less partition\n\njoined_df.write.format('noop').mode('overwrite').save()"}, {"cell_type": "code", "execution_count": 14, "id": "9704a51b-9530-4deb-8dfc-38c8c5ae029e", "metadata": {}, "outputs": [{"data": {"text/plain": "17653"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "customers_small_df.count()"}, {"cell_type": "code", "execution_count": null, "id": "cfb39d8b-be7f-4828-9c8d-008525952986", "metadata": {}, "outputs": [], "source": "spark.conf.set('spark.sql.adaptive.enabled','false')\n"}, {"cell_type": "code", "execution_count": 16, "id": "f6c92a64-1981-49fc-9401-bc0a71ed8242", "metadata": {}, "outputs": [], "source": "spark.conf.set('spark.sql.autoBroadcastJoinThreshold',-1)"}, {"cell_type": "code", "execution_count": 17, "id": "26ae929b-5094-419c-9020-b2d206a8f850", "metadata": {}, "outputs": [{"data": {"text/plain": "'-1'"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"}, {"cell_type": "code", "execution_count": 18, "id": "d4b36aaf-2795-48ee-a98c-f2aa53044953", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "orders_df.join(customers_small_df,on='customer_id',how='inner').write.format('noop').mode('overwrite').save()"}, {"cell_type": "markdown", "id": "0283b9fa-0de5-4502-9519-61c94429a8b8", "metadata": {}, "source": "Hi Mayank , I didn't receive any mail today so thought there was no class today! later on came to know class was there. Could you let me know what are we covering today?"}, {"cell_type": "code", "execution_count": 24, "id": "58abcbf2-cb08-4604-a56e-aef48b96a172", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+--------+\n|customer_id|cust_key|\n+-----------+--------+\n|    3194509|     109|\n|    8003925|     125|\n|    1946602|       2|\n|    8511049|      49|\n|    5013836|      36|\n|    5352874|      74|\n|    3676047|      47|\n|     663456|      56|\n|    5471172|     172|\n|    8298598|     198|\n|    2217320|     120|\n|    4796277|      77|\n|     766907|     107|\n|    3924592|     192|\n|    7014407|       7|\n|    4008694|      94|\n|    8248295|      95|\n|     372605|       5|\n|    7289017|      17|\n|    6406178|     178|\n+-----------+--------+\nonly showing top 20 rows\n\n+-----------+--------+\n|customer_id|cust_key|\n+-----------+--------+\n|          0|       0|\n|          1|       1|\n|          2|       2|\n|          3|       3|\n|          4|       4|\n|          5|       5|\n|          6|       6|\n|          7|       7|\n|          8|       8|\n|          9|       9|\n|         10|      10|\n|         11|      11|\n|         12|      12|\n|         13|      13|\n|         14|      14|\n|         15|      15|\n|         16|      16|\n|         17|      17|\n|         18|      18|\n|         19|      19|\n+-----------+--------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "orders_df.selectExpr('customer_id', 'customer_id % 200 as cust_key').show()\ncustomers_small_df.selectExpr('customer_id', 'customer_id % 200 as cust_key').show()"}, {"cell_type": "code", "execution_count": 26, "id": "b10743f1-6054-4928-8bce-842d259b5967", "metadata": {}, "outputs": [], "source": "spark.conf.set('spark.sql.autoBroadcastJoinThreshold','50m')"}, {"cell_type": "code", "execution_count": 27, "id": "8ba0e5c5-e0f4-481f-a502-cd8ab874fa0a", "metadata": {}, "outputs": [{"data": {"text/plain": "'50m'"}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"}, {"cell_type": "code", "execution_count": 28, "id": "1116debd-3c05-4bc8-a7be-ea03e1cf2735", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "orders_df.join(customers_small_df,on='customer_id',how='inner').write.format('noop').mode('overwrite').save()"}, {"cell_type": "code", "execution_count": null, "id": "4678cd7f-7e6f-4389-aeb5-4c288710e2f3", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "21776dac-9640-466f-a151-d1bec5325e80", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "dcd6bf6e-bc3d-48a6-9a07-56705552ea21", "metadata": {}, "source": "# Skew Join"}, {"cell_type": "code", "execution_count": 29, "id": "b95d80d1-7aad-475d-9e93-9e91c9f27ba6", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "(8767358, 8767358)"}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}], "source": "orders_df.count(),customers_df.count()"}, {"cell_type": "code", "execution_count": 34, "id": "3c7821af-7f7e-4cee-81ba-fb0d6567760e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n |-- registration_date: timestamp (nullable = true)\n |-- is_active: boolean (nullable = true)\n\n"}], "source": "customers_df.printSchema()"}, {"cell_type": "code", "execution_count": 35, "id": "2acd4fdc-a656-4b3d-8d7f-6f687d6f62af", "metadata": {}, "outputs": [], "source": "import pyspark.sql.functions as F"}, {"cell_type": "code", "execution_count": 33, "id": "0438fd2a-2672-430f-9beb-fd56983c29ec", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+-------+\n|     city|  count|\n+---------+-------+\n|Bangalore|1094195|\n|  Chennai|1095052|\n|   Mumbai|1095815|\n|Ahmedabad|1097162|\n|  Kolkata|1096777|\n|     Pune|1095748|\n|    Delhi|1096183|\n|Hyderabad|1096426|\n+---------+-------+\n\n"}], "source": "customers_df.groupBy('city').count().show()"}, {"cell_type": "code", "execution_count": 38, "id": "9b810156-2921-4f91-916b-53e738ea3dd8", "metadata": {}, "outputs": [], "source": "skew_factor = 1000"}, {"cell_type": "code", "execution_count": 39, "id": "01f740f3-3af1-45bd-8966-fbb12138ea1d", "metadata": {}, "outputs": [], "source": "mumbai_df = customers_df.filter(F.col('city')=='Mumbai')"}, {"cell_type": "code", "execution_count": 41, "id": "e7d41354-f8fa-41e9-b366-e0a4f8d93f6a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "1095815"}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}], "source": "mumbai_df.count()"}, {"cell_type": "code", "execution_count": 42, "id": "aaca09de-fad2-4d3b-940f-775508e1f3c3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "1095815000"}, "execution_count": 42, "metadata": {}, "output_type": "execute_result"}], "source": "# Generate multiple copies of Mumbai's data using an array expansion trick\nreplicated_mumbai_df = mumbai_df.withColumn(\"repeat\", F.explode(F.array([F.lit(i) for i in range(skew_factor)])))\nreplicated_mumbai_df.count()"}, {"cell_type": "code", "execution_count": 44, "id": "32afa9e0-64db-4e65-8c8c-966d3216b662", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----------+\n|     city|     count|\n+---------+----------+\n|Bangalore|   1094195|\n|  Chennai|   1095052|\n|   Mumbai|2192725815|\n|Ahmedabad|   1097162|\n|  Kolkata|   1096777|\n|     Pune|   1095748|\n|    Delhi|   1096183|\n|Hyderabad|   1096426|\n+---------+----------+\n\n"}], "source": "\n\n# Adjust customer_id to avoid duplicates\nreplicated_mumbai_df = replicated_mumbai_df.withColumn(\"customer_id\",(F.col(\"customer_id\") + F.monotonically_increasing_id()) % (10**8)).drop(\"repeat\")\n# Combine the skewed Mumbai data back into the main dataset\ncustomers_big_customers_dfskewed_df = customers_df.union(replicated_mumbai_df)\n# Verify the new distribution\ncustomers_big_customers_dfskewed_df.groupBy(\"city\").count().show()"}, {"cell_type": "code", "execution_count": 45, "id": "d2361abe-df57-42ed-a16d-e54d866137d7", "metadata": {}, "outputs": [], "source": "spark.conf.set('spark.sql.autoBroadcastJoinThreshold',-1)\nspark.conf.set('spark.sql.adaptive.enabled','false')"}, {"cell_type": "code", "execution_count": 46, "id": "670c1f20-cb33-4d97-b058-91202452f777", "metadata": {}, "outputs": [], "source": "city_data = [\n    (\"Mumbai\", \"High Population\"),\n    (\"Bangalore\", \"IT Hub\"),\n    (\"Hyderabad\", \"Tech City\"),\n    (\"Delhi\", \"Capital\"),\n    (\"Chennai\", \"Coastal City\"),\n    (\"Pune\", \"Education Hub\"),\n    (\"Ahmedabad\", \"Business Center\"),\n    (\"Kolkata\", \"Cultural City\"),\n]\n\ncity_columns = [\"city\", \"description\"]\ncity_df = spark.createDataFrame(city_data, city_columns)\n"}, {"cell_type": "code", "execution_count": 47, "id": "45b00bd2-3192-4f75-bcd3-ed147eb7d023", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "8"}, "execution_count": 47, "metadata": {}, "output_type": "execute_result"}], "source": "city_df.count()"}, {"cell_type": "code", "execution_count": 48, "id": "77126b3a-7183-4759-89cc-4a2aaff2061d", "metadata": {}, "outputs": [], "source": "joined_df = customers_big_customers_dfskewed_df.join(city_df,'city','inner')"}, {"cell_type": "code", "execution_count": 49, "id": "53a91a6a-1bcd-4d58-9d94-5b7b8d9564df", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/15 05:59:27 ERROR OverwriteByExpressionExec: Data source write support org.apache.spark.sql.execution.datasources.noop.NoopBatchWrite$@ccc9d74 is aborting.\n25/03/15 05:59:27 ERROR OverwriteByExpressionExec: Data source write support org.apache.spark.sql.execution.datasources.noop.NoopBatchWrite$@ccc9d74 aborted.\n"}, {"ename": "Py4JJavaError", "evalue": "An error occurred while calling o1235.save.\n: org.apache.spark.SparkException: Writing job aborted\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:766)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:262)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:262)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:318)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job 36 cancelled because killed via the Web UI\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2599)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageCancellation$1(DAGScheduler.scala:2588)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2869)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)\n\t... 44 more\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_17143/1416424613.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'noop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1235.save.\n: org.apache.spark.SparkException: Writing job aborted\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:766)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:409)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:353)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:262)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:332)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:331)\n\tat org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:262)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:318)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job 36 cancelled because killed via the Web UI\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2599)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageCancellation$1(DAGScheduler.scala:2588)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2869)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:377)\n\t... 44 more\n"]}], "source": "joined_df.write.format('noop').mode('overwrite').save()"}, {"cell_type": "markdown", "id": "526d168b-55a5-4cc7-8fb9-91e1e64ece0c", "metadata": {}, "source": "explain the term skewness with example"}, {"cell_type": "markdown", "id": "67250b55-3151-4133-bbfb-70983be40bb3", "metadata": {}, "source": "Our executor has storage memory as well, right? Why can\u2019t the job get successful by spilling data to disk?"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}