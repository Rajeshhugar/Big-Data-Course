{"cells": [{"cell_type": "code", "execution_count": 5, "id": "2d3448b5-32ec-49de-9ab2-bd3f757c880d", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, regexp_replace, trim, upper, lower, round, datediff, current_date\nimport pandas as pd\nimport mysql.connector"}, {"cell_type": "code", "execution_count": 1, "id": "e02f2b95-505e-496f-a3ee-f94bdaaaf1c2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/22 04:14:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n    .appName(\"CreditCardApproval_Cleaning\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n"}, {"cell_type": "code", "execution_count": 4, "id": "a92ea3d8-a3f7-4888-99c0-844f11dd58e1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: mysql-connector in /opt/conda/miniconda3/lib/python3.10/site-packages (2.2.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "!pip install mysql-connector"}, {"cell_type": "code", "execution_count": 6, "id": "60f9347d-484a-4fb4-ab40-175c7b910ea1", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/22 04:15:32 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.128.0.4:35164 is closed\n25/03/22 04:15:32 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 0 from block manager BlockManagerId(1, my-cluster-w-0.us-central1-c.c.celtic-science-452211-u6.internal, 35911, None)\njava.io.IOException: Connection from /10.128.0.4:35164 closed\n\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277) ~[netty-handler-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat java.lang.Thread.run(Thread.java:829) ~[?:?]\n/opt/conda/miniconda3/lib/python3.10/site-packages/pandas/io/sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n  warnings.warn(\n"}], "source": "gcs_bucket = \"dataproc-staging-us-central1-458263062208-tw36mmqt\"\ngcs_data_path = f\"gs://{gcs_bucket}/notebooks/jupyter/jupyter/Big Data Class Notebooks/Project/Project2/Data/application_record.csv\"\n\napplication_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(gcs_data_path)\n\nmysql_config = {\n    'host': '34.122.208.22',\n    'user': 'root',\n    'password': 'BigData@12345',\n    'database': 'loan_data',\n    'table_name': 'credit_record'\n}\n\nmysql_conn = mysql.connector.connect(\n    host=mysql_config[\"host\"],\n    user=mysql_config[\"user\"],\n    password=mysql_config[\"password\"],\n    database=mysql_config[\"database\"]\n)\n\n\nquery = f\"SELECT * FROM {mysql_config['table_name']}\"\npandas_df = pd.read_sql(query, mysql_conn) # If you try to read via Spark JDBC, you will face issues. That is why using Pandas \nmysql_conn.close()\n\n# IN the project as well, follow the same and you can have upto 5 mb file in SQL server\n\ncredit_df = spark.createDataFrame(pandas_df)"}, {"cell_type": "code", "execution_count": null, "id": "4b4e1578-a75f-431f-9d3e-546c7768ec64", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 7, "id": "440ce6d1-339a-490c-8c8d-c40e294781f7", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/22 04:16:31 WARN TaskSetManager: Stage 4 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n[Stage 4:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+--------------+------+\n|     ID|MONTHS_BALANCE|STATUS|\n+-------+--------------+------+\n|5001711|            -3|     0|\n|5001711|            -2|     0|\n|5001711|            -1|     0|\n|5001711|             0|     X|\n|5001712|           -18|     0|\n+-------+--------------+------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "credit_df.show(5)"}, {"cell_type": "code", "execution_count": 8, "id": "ca77569b-fdc8-4b5d-8dc3-85de9a681d72", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           null|            2.0|\n|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           null|            2.0|\n|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|\n|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\nonly showing top 5 rows\n\n"}], "source": "application_df.show(5)"}, {"cell_type": "code", "execution_count": 9, "id": "acc24831-3137-491a-a0b2-031eac4815ac", "metadata": {}, "outputs": [], "source": "# Handing Missing and Inconsistent Data"}, {"cell_type": "code", "execution_count": 10, "id": "5d13ec65-52d8-44a8-97a2-b134a1216ea4", "metadata": {}, "outputs": [], "source": "\n# Drop completely empty rows\napplication_df = application_df.dropna(how=\"all\")\ncredit_df = credit_df.dropna(how=\"all\")\n\n# Fill missing values\napplication_df = application_df.fillna({\"OCCUPATION_TYPE\": \"Unknown\"})\ncredit_df = credit_df.fillna({\"STATUS\": \"X\"})\n\n# Handling incorrect data (negative days should be converted to positive)\napplication_df = application_df.withColumn(\"DAYS_BIRTH\", col(\"DAYS_BIRTH\") * -1)\napplication_df = application_df.withColumn(\"DAYS_EMPLOYED\", when(col(\"DAYS_EMPLOYED\") < 0, col(\"DAYS_EMPLOYED\") * -1).otherwise(col(\"DAYS_EMPLOYED\")))\n"}, {"cell_type": "code", "execution_count": 11, "id": "a23d1866-b36c-4511-ae88-6e7e426a8cb1", "metadata": {}, "outputs": [], "source": "# Trim whitespace\napplication_df = application_df.select([trim(col(c)).alias(c) for c in application_df.columns])\n\n# Standardize categorical values\napplication_df = application_df.withColumn(\"CODE_GENDER\", upper(col(\"CODE_GENDER\")))\napplication_df = application_df.withColumn(\"NAME_FAMILY_STATUS\", lower(col(\"NAME_FAMILY_STATUS\")))\n\n# Convert amount fields to float & round to 2 decimals\napplication_df = application_df.withColumn(\"AMT_INCOME_TOTAL\", round(col(\"AMT_INCOME_TOTAL\"), 2))"}, {"cell_type": "code", "execution_count": 12, "id": "e9bf7a51-67a6-4823-922f-0016d22f9aba", "metadata": {}, "outputs": [], "source": "# Age in years\napplication_df = application_df.withColumn(\"AGE\", round(col(\"DAYS_BIRTH\") / 365, 0))\n\n# Employment length in years\napplication_df = application_df.withColumn(\"EMPLOYMENT_YEARS\", round(col(\"DAYS_EMPLOYED\") / 365, 0))"}, {"cell_type": "code", "execution_count": 15, "id": "43e86bee-c949-4f8a-978d-383486e67bee", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+----------+\n| AGE|DAYS_BIRTH|\n+----+----------+\n|33.0|     12005|\n|33.0|     12005|\n|59.0|     21474|\n|52.0|     19110|\n|52.0|     19110|\n|52.0|     19110|\n|52.0|     19110|\n|62.0|     22464|\n|62.0|     22464|\n|62.0|     22464|\n|46.0|     16872|\n|46.0|     16872|\n|46.0|     16872|\n|49.0|     17778|\n|49.0|     17778|\n|49.0|     17778|\n|49.0|     17778|\n|49.0|     17778|\n|49.0|     17778|\n|29.0|     10669|\n+----+----------+\nonly showing top 20 rows\n\n"}], "source": "application_df.select('AGE','DAYS_BIRTH').show()"}, {"cell_type": "code", "execution_count": null, "id": "20286fca-4a30-48aa-a10c-be0ce8b8a7c5", "metadata": {}, "outputs": [], "source": "GCS/jupyter/Big Data Class Notebooks/Project/Project2/Project Part 2/Data Cleaning & Transformation.ipynb"}, {"cell_type": "code", "execution_count": null, "id": "a7164cd6-76dc-44ad-8dc8-135c37752b33", "metadata": {}, "outputs": [], "source": "GCS/jupyter/Big Data Class Notebooks/Project/Project2/Project Part 2/cleaned_data/untitled.txt"}, {"cell_type": "code", "execution_count": 17, "id": "4cc75327-8888-4472-849a-88dcc09176a0", "metadata": {}, "outputs": [{"data": {"text/plain": "'dataproc-staging-us-central1-458263062208-tw36mmqt'"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "gcs_bucket"}, {"cell_type": "code", "execution_count": 18, "id": "bdd8ecc2-abe5-470c-bd51-8938eea699c0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/22 04:28:13 WARN TaskSetManager: Stage 12 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r"}], "source": "cleaned_gcs_path = f\"gs://{gcs_bucket}/notebooks/jupyter/jupyter/Big Data Class Notebooks/Project/Project2/Project_5_steps/cleaned_data/\"\napplication_df.write.mode(\"overwrite\").parquet(cleaned_gcs_path + \"application_record_cleaned.parquet\")\ncredit_df.write.mode(\"overwrite\").parquet(cleaned_gcs_path + \"credit_record_cleaned.parquet\")"}, {"cell_type": "code", "execution_count": null, "id": "9a10c788-948a-4d7b-8c77-cf567c36181f", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}