{"cells": [{"cell_type": "code", "execution_count": 29, "id": "4539ee9c-6ed8-4f3c-af6e-b09749ef4d9e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/21 18:03:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n/opt/conda/miniconda3/lib/python3.10/site-packages/pandas/io/sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n  warnings.warn(\n"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- ID: integer (nullable = true)\n |-- CODE_GENDER: string (nullable = true)\n |-- FLAG_OWN_CAR: string (nullable = true)\n |-- FLAG_OWN_REALTY: string (nullable = true)\n |-- CNT_CHILDREN: integer (nullable = true)\n |-- AMT_INCOME_TOTAL: double (nullable = true)\n |-- NAME_INCOME_TYPE: string (nullable = true)\n |-- NAME_EDUCATION_TYPE: string (nullable = true)\n |-- NAME_FAMILY_STATUS: string (nullable = true)\n |-- NAME_HOUSING_TYPE: string (nullable = true)\n |-- DAYS_BIRTH: integer (nullable = true)\n |-- DAYS_EMPLOYED: integer (nullable = true)\n |-- FLAG_MOBIL: integer (nullable = true)\n |-- FLAG_WORK_PHONE: integer (nullable = true)\n |-- FLAG_PHONE: integer (nullable = true)\n |-- FLAG_EMAIL: integer (nullable = true)\n |-- OCCUPATION_TYPE: string (nullable = true)\n |-- CNT_FAM_MEMBERS: double (nullable = true)\n\nroot\n |-- ID: string (nullable = true)\n |-- MONTHS_BALANCE: long (nullable = true)\n |-- STATUS: string (nullable = true)\n\n+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           null|            2.0|\n|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           null|            2.0|\n|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|\n|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "25/03/21 18:03:56 WARN TaskSetManager: Stage 21 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n"}, {"name": "stdout", "output_type": "stream", "text": "+-------+--------------+------+\n|     ID|MONTHS_BALANCE|STATUS|\n+-------+--------------+------+\n|5001711|            -3|     0|\n|5001711|            -2|     0|\n|5001711|            -1|     0|\n|5001711|             0|     X|\n|5001712|           -18|     0|\n+-------+--------------+------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "25/03/21 18:03:58 WARN TaskSetManager: Stage 25 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n"}, {"name": "stdout", "output_type": "stream", "text": "Application Records: 438557\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Credit Records: 1048575\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-----------------+-----------+------------+---------------+-------------------+------------------+--------------------+--------------------+------------------+-----------------+-------------------+-----------------+----------+------------------+-------------------+-------------------+--------------------+-----------------+\n|summary|               ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|       CNT_CHILDREN|  AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|         DAYS_BIRTH|    DAYS_EMPLOYED|FLAG_MOBIL|   FLAG_WORK_PHONE|         FLAG_PHONE|         FLAG_EMAIL|     OCCUPATION_TYPE|  CNT_FAM_MEMBERS|\n+-------+-----------------+-----------+------------+---------------+-------------------+------------------+--------------------+--------------------+------------------+-----------------+-------------------+-----------------+----------+------------------+-------------------+-------------------+--------------------+-----------------+\n|  count|           438557|     438557|      438557|         438557|             438557|            438557|              438557|              438557|            438557|           438557|             438557|           438557|    438557|            438557|             438557|             438557|              304354|           438557|\n|   mean|6022176.269841776|       null|        null|           null|0.42739028222101116| 187524.2860095039|                null|                null|              null|             null|-15997.904648654565|60563.67532840657|       1.0|0.2061328402009317|0.28777103090362255|0.10820714297115312|                null|2.194465029631268|\n| stddev|571637.0232571006|       null|        null|           null|  0.724882213552146|110086.85306622987|                null|                null|              null|             null|  4185.030006794112|138767.7996466682|       0.0|0.4045274595500885|  0.452724344414615| 0.3106422012826674|                null|0.897207330056299|\n|    min|          5008804|          F|           N|              N|                  0|           26100.0|Commercial associate|     Academic degree|    Civil marriage|  Co-op apartment|             -25201|           -17531|         1|                 0|                  0|                  0|         Accountants|              1.0|\n|    max|          7999952|          M|           Y|              Y|                 19|         6750000.0|             Working|Secondary / secon...|             Widow|     With parents|              -7489|           365243|         1|                 1|                  1|                  1|Waiters/barmen staff|             20.0|\n+-------+-----------------+-----------+------------+---------------+-------------------+------------------+--------------------+--------------------+------------------+-----------------+-------------------+-----------------+----------+------------------+-------------------+-------------------+--------------------+-----------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "25/03/21 18:04:40 WARN TaskSetManager: Stage 31 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n[Stage 31:>                                                         (0 + 2) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-----------------+-------------------+-------------------+\n|summary|               ID|     MONTHS_BALANCE|             STATUS|\n+-------+-----------------+-------------------+-------------------+\n|  count|          1048575|            1048575|            1048575|\n|   mean|5068286.424673486|-19.136998307226474|0.05824863961501482|\n| stddev|46150.57850528666| 14.023497688326547| 0.3949878642052498|\n|    min|          5001711|                -60|                  0|\n|    max|          5150487|                  0|                  X|\n+-------+-----------------+-------------------+-------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "import pandas as pd\nfrom pyspark.sql import SparkSession\nimport mysql.connector\n\n# Initialize Spark Session\nspark = SparkSession.builder \\\n    .appName(\"CreditCardApprovalPipeline\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Load Application Data from GCS\ngcs_bucket = \"dataproc-staging-us-central1-458263062208-tw36mmqt\"\ngcs_data_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/loan_prediction_data/application_record.csv\"\n\napplication_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(gcs_data_path)\n\n# Load Credit Record Data from MySQL using Pandas\nmysql_config = {\n    \"host\": \"34.46.229.160\",\n    \"user\": \"root\",\n    \"password\": \"b)p[/&GH-o|B]6u+\",\n    \"database\": \"loan_data\",\n    \"table\": \"credit_record_final\"\n}\n\nmysql_conn = mysql.connector.connect(\n    host=mysql_config[\"host\"],\n    user=mysql_config[\"user\"],\n    password=mysql_config[\"password\"],\n    database=mysql_config[\"database\"]\n)\n\nquery = f\"SELECT * FROM {mysql_config['table']}\"\npandas_df = pd.read_sql(query, mysql_conn)\nmysql_conn.close()\n\ncredit_df = spark.createDataFrame(pandas_df)\n\n# Data Exploration\napplication_df.printSchema()\ncredit_df.printSchema()\n\napplication_df.show(5)\ncredit_df.show(5)\n\nprint(f\"Application Records: {application_df.count()}\")\nprint(f\"Credit Records: {credit_df.count()}\")\n\napplication_df.describe().show()\ncredit_df.describe().show()\n"}, {"cell_type": "code", "execution_count": 31, "id": "b646b4df-321a-4c4a-950f-9a776d9c35a4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/opt/conda/miniconda3/lib/python3.10/site-packages/pandas/io/sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n  warnings.warn(\n25/03/21 18:09:41 WARN TaskSetManager: Stage 39 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- ID: string (nullable = true)\n |-- CODE_GENDER: string (nullable = true)\n |-- FLAG_OWN_CAR: string (nullable = true)\n |-- FLAG_OWN_REALTY: string (nullable = true)\n |-- CNT_CHILDREN: string (nullable = true)\n |-- AMT_INCOME_TOTAL: double (nullable = true)\n |-- NAME_INCOME_TYPE: string (nullable = true)\n |-- NAME_EDUCATION_TYPE: string (nullable = true)\n |-- NAME_FAMILY_STATUS: string (nullable = true)\n |-- NAME_HOUSING_TYPE: string (nullable = true)\n |-- DAYS_BIRTH: string (nullable = true)\n |-- DAYS_EMPLOYED: string (nullable = true)\n |-- FLAG_MOBIL: string (nullable = true)\n |-- FLAG_WORK_PHONE: string (nullable = true)\n |-- FLAG_PHONE: string (nullable = true)\n |-- FLAG_EMAIL: string (nullable = true)\n |-- OCCUPATION_TYPE: string (nullable = false)\n |-- CNT_FAM_MEMBERS: string (nullable = true)\n |-- AGE: double (nullable = true)\n |-- EMPLOYMENT_YEARS: double (nullable = true)\n\nroot\n |-- ID: string (nullable = true)\n |-- MONTHS_BALANCE: long (nullable = true)\n |-- STATUS: string (nullable = false)\n\n+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+----+----------------+\n|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS| AGE|EMPLOYMENT_YEARS|\n+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+----+----------------+\n|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      civil marriage| Rented apartment|     12005|         4542|         1|              1|         0|         0|        Unknown|            2.0|33.0|            12.0|\n|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      civil marriage| Rented apartment|     12005|         4542|         1|              1|         0|         0|        Unknown|            2.0|33.0|            12.0|\n|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             married|House / apartment|     21474|         1134|         1|              0|         0|         0| Security staff|            2.0|59.0|             3.0|\n|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|single / not married|House / apartment|     19110|         3051|         1|              0|         1|         1|    Sales staff|            1.0|52.0|             8.0|\n|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|single / not married|House / apartment|     19110|         3051|         1|              0|         1|         1|    Sales staff|            1.0|52.0|             8.0|\n+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+----+----------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "25/03/21 18:09:44 WARN TaskSetManager: Stage 41 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n"}, {"name": "stdout", "output_type": "stream", "text": "+-------+--------------+------+\n|     ID|MONTHS_BALANCE|STATUS|\n+-------+--------------+------+\n|5001711|            -3|     0|\n|5001711|            -2|     0|\n|5001711|            -1|     0|\n|5001711|             0|     X|\n|5001712|           -18|     0|\n+-------+--------------+------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "25/03/21 18:09:45 WARN TaskSetManager: Stage 45 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n"}, {"name": "stdout", "output_type": "stream", "text": "Cleaned Application Records: 438557\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Cleaned Credit Records: 1048575\n"}, {"name": "stderr", "output_type": "stream", "text": "25/03/21 18:10:29 WARN TaskSetManager: Stage 51 contains a task of very large size (9759 KiB). The maximum recommended task size is 1000 KiB.\n"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-----------------+-----------+------------+---------------+-------------------+------------------+--------------------+--------------------+------------------+-----------------+------------------+------------------+----------+------------------+-------------------+-------------------+--------------------+-----------------+------------------+------------------+\n|summary|               ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|       CNT_CHILDREN|  AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|        DAYS_BIRTH|     DAYS_EMPLOYED|FLAG_MOBIL|   FLAG_WORK_PHONE|         FLAG_PHONE|         FLAG_EMAIL|     OCCUPATION_TYPE|  CNT_FAM_MEMBERS|               AGE|  EMPLOYMENT_YEARS|\n+-------+-----------------+-----------+------------+---------------+-------------------+------------------+--------------------+--------------------+------------------+-----------------+------------------+------------------+----------+------------------+-------------------+-------------------+--------------------+-----------------+------------------+------------------+\n|  count|           438557|     438557|      438557|         438557|             438557|            438557|              438557|              438557|            438557|           438557|            438557|            438557|    438557|            438557|             438557|             438557|              438557|           438557|            438557|            438557|\n|   mean|6022176.269841776|       null|        null|           null|0.42739028222101116| 187524.2860095039|                null|                null|              null|             null|15997.904648654565|64908.680360819686|       1.0|0.2061328402009317|0.28777103090362255|0.10820714297115312|                null|2.194465029631268| 43.83396000975928|177.89758685872076|\n| stddev|571637.0232571006|       null|        null|           null|  0.724882213552146|110086.85306622987|                null|                null|              null|             null| 4185.030006794112|136789.33788405915|       0.0|0.4045274595500885|  0.452724344414615| 0.3106422012826674|                null|0.897207330056299|11.472764868263813|374.88779912019726|\n|    min|          5008804|          F|           N|              N|                  0|           26100.0|Commercial associate|     Academic degree|    civil marriage|  Co-op apartment|             10000|               100|         1|                 0|                  0|                  0|         Accountants|              1.0|              21.0|               0.0|\n|    max|          7999952|          M|           Y|              Y|                  9|         6750000.0|             Working|Secondary / secon...|             widow|     With parents|              9999|              9997|         1|                 1|                  1|                  1|Waiters/barmen staff|              9.0|              69.0|            1001.0|\n+-------+-----------------+-----------+------------+---------------+-------------------+------------------+--------------------+--------------------+------------------+-----------------+------------------+------------------+----------+------------------+-------------------+-------------------+--------------------+-----------------+------------------+------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 51:>                                                         (0 + 2) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-----------------+-------------------+-------------------+\n|summary|               ID|     MONTHS_BALANCE|             STATUS|\n+-------+-----------------+-------------------+-------------------+\n|  count|          1048575|            1048575|            1048575|\n|   mean|5068286.424673486|-19.136998307226474|0.05824863961501482|\n| stddev|46150.57850528666| 14.023497688326547| 0.3949878642052498|\n|    min|          5001711|                -60|                  0|\n|    max|          5150487|                  0|                  X|\n+-------+-----------------+-------------------+-------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, regexp_replace, trim, upper, lower, round, datediff, current_date\nimport pandas as pd\nimport mysql.connector\n\n# ==============================\n# 1. SETUP SPARK SESSION\n# ==============================\n\nspark = SparkSession.builder \\\n    .appName(\"CreditCardApproval_Cleaning\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# ==============================\n# 2. LOAD DATA FROM GCS & MYSQL\n# ==============================\n\n# GCS Bucket Path\ngcs_bucket = \"dataproc-staging-us-central1-458263062208-tw36mmqt\"\ngcs_data_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/loan_prediction_data/application_record.csv\"\n\n# Read CSV from GCS\napplication_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(gcs_data_path)\n\n# Load Credit Record Data from MySQL\nmysql_config = {\n    \"host\": \"34.46.229.160\",\n    \"user\": \"root\",\n    \"password\": \"b)p[/&GH-o|B]6u+\",\n    \"database\": \"loan_data\",\n    \"table\": \"credit_record_final\"\n}\n\nmysql_conn = mysql.connector.connect(\n    host=mysql_config[\"host\"],\n    user=mysql_config[\"user\"],\n    password=mysql_config[\"password\"],\n    database=mysql_config[\"database\"]\n)\n\nquery = f\"SELECT * FROM {mysql_config['table']}\"\npandas_df = pd.read_sql(query, mysql_conn)\nmysql_conn.close()\n\ncredit_df = spark.createDataFrame(pandas_df)\n\n# ==============================\n# 3. HANDLING MISSING & INCONSISTENT DATA\n# ==============================\n\n# Drop completely empty rows\napplication_df = application_df.dropna(how=\"all\")\ncredit_df = credit_df.dropna(how=\"all\")\n\n# Fill missing values\napplication_df = application_df.fillna({\"OCCUPATION_TYPE\": \"Unknown\"})\ncredit_df = credit_df.fillna({\"STATUS\": \"X\"})\n\n# Handling incorrect data (negative days should be converted to positive)\napplication_df = application_df.withColumn(\"DAYS_BIRTH\", col(\"DAYS_BIRTH\") * -1)\napplication_df = application_df.withColumn(\"DAYS_EMPLOYED\", when(col(\"DAYS_EMPLOYED\") < 0, col(\"DAYS_EMPLOYED\") * -1).otherwise(col(\"DAYS_EMPLOYED\")))\n\n# ==============================\n# 4. STANDARDIZING DATA FORMATS\n# ==============================\n\n# Trim whitespace\napplication_df = application_df.select([trim(col(c)).alias(c) for c in application_df.columns])\n\n# Standardize categorical values\napplication_df = application_df.withColumn(\"CODE_GENDER\", upper(col(\"CODE_GENDER\")))\napplication_df = application_df.withColumn(\"NAME_FAMILY_STATUS\", lower(col(\"NAME_FAMILY_STATUS\")))\n\n# Convert amount fields to float & round to 2 decimals\napplication_df = application_df.withColumn(\"AMT_INCOME_TOTAL\", round(col(\"AMT_INCOME_TOTAL\"), 2))\n\n# ==============================\n# 5. CREATING NEW FEATURES\n# ==============================\n\n# Age in years\napplication_df = application_df.withColumn(\"AGE\", round(col(\"DAYS_BIRTH\") / 365, 0))\n\n# Employment length in years\napplication_df = application_df.withColumn(\"EMPLOYMENT_YEARS\", round(col(\"DAYS_EMPLOYED\") / 365, 0))\n\n# Customer's credit history length\n\n# ==============================\n# 6. SAVING CLEANED DATA\n# ==============================\n\ncleaned_gcs_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/cleaned_data/\"\napplication_df.write.mode(\"overwrite\").parquet(cleaned_gcs_path + \"application_record_cleaned.parquet\")\ncredit_df.write.mode(\"overwrite\").parquet(cleaned_gcs_path + \"credit_record_cleaned.parquet\")\n\n# ==============================\n# 7. DATA EXPLORATION POST CLEANING\n# ==============================\n\napplication_df.printSchema()\ncredit_df.printSchema()\n\napplication_df.show(5)\ncredit_df.show(5)\n\nprint(f\"Cleaned Application Records: {application_df.count()}\")\nprint(f\"Cleaned Credit Records: {credit_df.count()}\")\n\napplication_df.describe().show()\ncredit_df.describe().show()\n"}, {"cell_type": "code", "execution_count": 33, "id": "d2055377-e959-4ba1-a872-61252bb7609c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "ROC-AUC: 0.5221\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# ==============================\n# 1. SETUP SPARK SESSION\n# ==============================\n\nspark = SparkSession.builder \\\n    .appName(\"CreditCardApproval_Integration_Modeling\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# ==============================\n# 2. LOAD CLEANED DATA FROM GCS\n# ==============================\n\ngcs_bucket = \"dataproc-staging-us-central1-458263062208-tw36mmqt\"\napplication_cleaned_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/cleaned_data/application_record_cleaned.parquet\"\ncredit_cleaned_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/cleaned_data/credit_record_cleaned.parquet\"\n\n# Read cleaned data from GCS\napplication_df = spark.read.parquet(application_cleaned_path)\ncredit_df = spark.read.parquet(credit_cleaned_path)\n\n# ==============================\n# 3. DATA INTEGRATION\n# ==============================\n\n# Join on 'ID'\nintegrated_df = application_df.join(credit_df, on='ID', how='inner')\n\n# ==============================\n# 4. BASIC MACHINE LEARNING MODEL\n# ==============================\n\n# Define label (Creating a target variable if not present)\nif 'TARGET' not in integrated_df.columns:\n    integrated_df = integrated_df.withColumn('TARGET', when(col('STATUS') == '2', 1).otherwise(0))\n\n# Select a few numerical columns for ML model\nfeature_columns = ['AMT_INCOME_TOTAL', 'AGE']\n\n# Assemble features into a single column\nassembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\nintegrated_df = assembler.transform(integrated_df)\n\n# Split data into training and testing sets\ntrain_df, test_df = integrated_df.randomSplit([0.8, 0.2], seed=42)\n\n# Train a basic Logistic Regression model\nlr = LogisticRegression(featuresCol='features', labelCol='TARGET')\nlr_model = lr.fit(train_df)\n\n# Make predictions\npredictions = lr_model.transform(test_df)\n\n# Evaluate using AUC\nevaluator = BinaryClassificationEvaluator(labelCol='TARGET', metricName='areaUnderROC')\nroc_auc = evaluator.evaluate(predictions)\n\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\n\n# ==============================\n# 5. SAVE INTEGRATED DATA\n# ==============================\n\nintegrated_data_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/integrated_data/\"\nintegrated_df.write.mode(\"overwrite\").parquet(integrated_data_path + \"integrated_data.parquet\")\n"}, {"cell_type": "code", "execution_count": null, "id": "e098a1ef-398a-4007-a09e-4b258bc2f5fc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/21 18:20:13 WARN CacheManager: Asked to cache already cached data.\n                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport mysql.connector\n\n# ==============================\n# 1. SETUP SPARK SESSION\n# ==============================\n\nspark = SparkSession.builder \\\n    .appName(\"CreditCardApproval_Optimization_Serving\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# ==============================\n# 2. LOAD INTEGRATED DATA\n# ==============================\n\ngcs_bucket = \"dataproc-staging-us-central1-458263062208-tw36mmqt\"\nintegrated_data_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/integrated_data/integrated_data.parquet\"\n\n# Read integrated data\ndf = spark.read.parquet(integrated_data_path)\n\n# ==============================\n# 3. PERFORMANCE OPTIMIZATION\n# ==============================\n\n# Repartition for better parallel processing\ndf = df.repartition(10, \"ID\")\n\n# Cache the dataset for faster reuse\ndf.cache()\n\n# Enable Predicate Pushdown for optimized filtering\ndf = df.filter(col(\"AMT_INCOME_TOTAL\") > 10000)\n\n# ==============================\n# 4. DATA SERVING (WRITE TO GCS AND MYSQL)\n# ==============================\n\n# Save final data to GCS in Parquet format\nfinal_gcs_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/final_data/\"\ndf.write.mode(\"overwrite\").parquet(final_gcs_path + \"final_data.parquet\")\n\n# Save data to MySQL\nmysql_config = {\n    \"host\": \"34.46.229.160\",\n    \"user\": \"root\",\n    \"password\": \"b)p[/&GH-o|B]6u+\",\n    \"database\": \"loan_data\",\n    \"table\": \"final_credit_data\"\n}\n\n# Convert Spark DataFrame to Pandas\npandas_df = df.toPandas()\npandas_df = pandas_df[:10]\n\n# Connect to MySQL\nmysql_conn = mysql.connector.connect(\n    host=mysql_config[\"host\"],\n    user=mysql_config[\"user\"],\n    password=mysql_config[\"password\"],\n    database=mysql_config[\"database\"]\n)\n\ncursor = mysql_conn.cursor()\n\n# Create table if not exists\ncreate_table_query = f\"\"\"\nCREATE TABLE IF NOT EXISTS {mysql_config['table']} (\n    ID VARCHAR(50),\n    AMT_INCOME_TOTAL FLOAT,\n    AGE INT,\n    CNT_FAM_MEMBERS INT,\n    TARGET INT\n);\n\"\"\"\ncursor.execute(create_table_query)\n\n# Insert data\nfor _, row in pandas_df.iterrows():\n    cursor.execute(f\"\"\"\n        INSERT INTO {mysql_config['table']} (ID, AMT_INCOME_TOTAL, AGE, CNT_FAM_MEMBERS, TARGET)\n        VALUES (%s, %s, %s, %s, %s)\n        ON DUPLICATE KEY UPDATE TARGET=VALUES(TARGET);\n    \"\"\", (row[\"ID\"], row[\"AMT_INCOME_TOTAL\"], row[\"AGE\"], row[\"CNT_FAM_MEMBERS\"], row[\"TARGET\"]))\n\nmysql_conn.commit()\ncursor.close()\nmysql_conn.close()\n\nprint(\"\u2705 Data Optimization & Serving Completed.\")\n"}, {"cell_type": "code", "execution_count": 1, "id": "19c16153-cce6-4a25-b2a1-5d620eb39513", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/21 18:29:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "\u2705 Data Optimization & Serving Completed. Final data saved to GCS.\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n# ==============================\n# 1. SETUP SPARK SESSION\n# ==============================\n\nspark = SparkSession.builder \\\n    .appName(\"CreditCardApproval_Optimization_Serving\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# ==============================\n# 2. LOAD INTEGRATED DATA\n# ==============================\n\ngcs_bucket = \"dataproc-staging-us-central1-458263062208-tw36mmqt\"\nintegrated_data_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/integrated_data/integrated_data.parquet\"\n\n# Read integrated data\ndf = spark.read.parquet(integrated_data_path)\n\n# ==============================\n# 3. PERFORMANCE OPTIMIZATION\n# ==============================\n\n# Repartition for better parallel processing\ndf = df.repartition(10, \"ID\")\n\n# Cache the dataset for faster reuse\ndf.cache()\n\n# Enable Predicate Pushdown for optimized filtering\ndf = df.filter(col(\"AMT_INCOME_TOTAL\") > 10000)\n\n# ==============================\n# 4. SAVE FINAL DATA TO GCS\n# ==============================\n\nfinal_gcs_path = f\"gs://{gcs_bucket}/notebooks/jupyter/Data/final_data/\"\ndf.write.mode(\"overwrite\").parquet(final_gcs_path + \"final_data.parquet\")\n\nprint(\"\u2705 Data Optimization & Serving Completed. Final data saved to GCS.\")\n"}, {"cell_type": "code", "execution_count": null, "id": "d5f127b4-abbd-4450-bf15-be1f61f5e937", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}