





!SET SPARK_LOCAL_HOSTNAME=localhost


import os
os.environ["PYSPARK_PYTHON"] = r"C:\Users\hugar\.conda\envs\pyspark_env\python.exe"
os.environ["PYSPARK_DRIVER_PYTHON"] = r"C:\Users\hugar\.conda\envs\pyspark_env\python.exe"



!set PYSPARK_DRIVER_PYTHON=C:\Python312\python.exe




# Import SparkSession
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("PySpark Basics") \
    .getOrCreate()


spark








# Sample data
data = [
    (1, "John Doe", "Bangalore", "2023-01-15", "152.75", "True"),
    (2, "Jane Smith", "Delhi", "2023-05-20", "89.50", "False"),
    (3, "Robert Brown", "Mumbai", "InvalidDate", "200.00", "True"),
    (4, "Linda White", "Kolkata", "2023-02-29", None, "yes"),  # Feb 29 invalid in 2023
    (5, "Mike Green", "Chennai", "2023-08-10", "NaN", "1"),  # NaN needs handling
    (6, "Sarah Blue", "Hyderabad", "InvalidDate", "300.40", "No")
]

# Define column names
columns = ["id", "name", "city", "date", "amount", "is_active"]

# Create DataFrame
df = spark.createDataFrame(data, schema=columns)

# Show the DataFrame
df.show()








import sys
print(sys.executable)



df.show()


!pip install --upgrade py4j



!pip show py4j



!java -version




!pip show pyspark








# Print the schema
df.printSchema()








df.id


df['id']


# Filter rows where id > 3
df.filter(df.id > 3).show()

# Add a new column with id multiplied by 2
df = df.withColumn("id_double", df.id * 2)
df.show()








from pyspark.sql.functions import *


# Convert name to uppercase
df = df.withColumn("name_upper", upper(df.name))
df.show()

# Filter rows where city starts with 'B'
df.filter(df.city.startswith("B")).show()








# Replace 'NaN' with null and cast to float
from pyspark.sql.functions import col
df = df.withColumn("amount", col("amount").cast("float"))
df.show()









from pyspark.sql.functions import to_date

# Add more date columns with different formats
df = df.withColumn("date_alt_format", to_date(df.date, "dd-MM-yyyy"))  # Alternate format
df = df.withColumn("date_with_time", to_date(df.date, "yyyy-MM-dd HH:mm:ss"))  # Date with time

# Convert valid dates and handle invalid dates
df = df.withColumn("parsed_date", to_date(df.date, "yyyy-MM-dd"))

df = df.withColumn("date_alt_format", to_date(df.parsed_date, "dd-MM-yyyy"))  # Alternate format
df = df.withColumn("date_with_time", to_date(df.parsed_date, "yyyy-MM-dd HH:mm:ss"))  # Date with time


# Show the DataFrame
df.show()

# Filter rows with valid dates
df.filter(df.parsed_date.isNotNull()).show()








from pyspark.sql.functions import lit, when

# Add a boolean column based on `is_active`
df = df.withColumn("is_active_bool", when(df.is_active.isin("True", "yes", "1"), True).otherwise(False))

# Add a constant column
df = df.withColumn("constant_col", lit("PySpark"))

# Add a calculated column (amount + 10)
df = df.withColumn("amount_plus_10", df.amount + 10)

# Show the final DataFrame
df.show()





spark.stop()
