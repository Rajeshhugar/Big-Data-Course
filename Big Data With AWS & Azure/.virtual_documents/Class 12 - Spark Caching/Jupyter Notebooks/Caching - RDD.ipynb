from pyspark.sql import SparkSession


spark = SparkSession.builder.appName('Caching_Big_File').enableHiveSupport().getOrCreate()


spark











customers_rdd = spark.sparkContext.textFile('/tmp/customers_500mb.csv')


customers_filtered = customers_rdd.filter(lambda row : 'Mumbai' in row)
customers_mapped = customers_filtered.map(lambda row : (row.split(',')[0],1))
customers_reduced = customers_mapped.reduceByKey(lambda x,y:x+y)





customers_reduced.count()


customers_reduced.count()


customers_reduced.cache()


customers_reduced.count()


customers_reduced.count()





customers_reduced.count()


customers_reduced.collect()


customers_final = customers_reduced.filter(lambda row: int(row[0])<500)


customers_final.collect()


customers_reduced.unpersist()





spark.stop()
