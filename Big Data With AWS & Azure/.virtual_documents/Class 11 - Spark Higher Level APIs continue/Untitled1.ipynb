from pyspark.sql import SparkSession

# Initialize SparkSession with Hive support
spark = SparkSession.builder \
    .appName("TableDemo") \
    .enableHiveSupport() \
    .getOrCreate()

# Read CSV from HDFS
hdfs_path = "/tmp/customers_1mb.csv"  # Update path as per your HDFS location
df = spark.read.option("header", True).csv(hdfs_path)

# Show Data
df.show(5)

# ---------------- Step 1: Create a Temporary Table (Session-based) ---------------- #
df.createOrReplaceTempView("temp_customers")

print("### Querying Temporary Table (Exists only in this session) ###")
spark.sql("SELECT * FROM temp_customers LIMIT 5").show()

# ---------------- Step 2: Create a Global Temporary Table (Accessible across sessions) ---------------- #
df.createOrReplaceGlobalTempView("global_customers")

print("### Querying Global Temporary Table ###")
spark.sql("SELECT * FROM global_temp.global_customers LIMIT 5").show()

# ---------------- Step 3: Create a Persistent Table (Stored in Hive Metastore) ---------------- #
spark.sql("DROP TABLE IF EXISTS customers_persistent")
df.write.mode("overwrite").saveAsTable("customers_persistent")

print("### Querying Persistent Table (Accessible across sessions and applications) ###")
spark.sql("SELECT * FROM customers_persistent LIMIT 5").show()

