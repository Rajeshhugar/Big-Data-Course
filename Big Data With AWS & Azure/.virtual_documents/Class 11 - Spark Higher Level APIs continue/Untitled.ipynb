from pyspark.sql import SparkSession


spark = SparkSession.builder \
.appName("Spark_Table") \
.getOrCreate()


df = spark.read \
.format('csv')\
.option('header','True')\
.option('inferSchema','True')\
.load('/tmp/customers_1mb.csv')


df.show()


df.createOrReplaceTempView('customers')


df.createTempView('customers')


spark.sql('select * from customers limit 5').show()


df.createOrReplaceGlobalTempView('global_customers')


spark.sql('select * from global_temp.global_customers limit 5').show()


spark2 = SparkSession.builder \
.appName("Spark_Table") \
.getOrCreate()


spark2.sql('select * from global_temp.global_customers limit 5').show()





spark.sql('show databases').show()


spark.sql('use default')


df.write.format('csv').saveAsTable('default.customers')


spark.sql('select * from default.customers limit 5').show()


spark.stop()





from pyspark.sql import SparkSession

# Initialize SparkSession with Hive support
spark = SparkSession.builder \
    .appName("TableDemo") \
    .enableHiveSupport() \
    .getOrCreate()

# Read CSV from HDFS
hdfs_path = "/tmp/customers_1mb.csv"  # Update path as per your HDFS location
df = spark.read.option("header", True).csv(hdfs_path)

# Show Data
df.show(5)

# ---------------- Step 1: Create a Temporary Table (Session-based) ---------------- #
df.createOrReplaceTempView("temp_customers")

print("### Querying Temporary Table (Exists only in this session) ###")
spark.sql("SELECT * FROM temp_customers LIMIT 5").show()

# ---------------- Step 2: Create a Global Temporary Table (Accessible across sessions) ---------------- #
df.createOrReplaceGlobalTempView("global_customers")

print("### Querying Global Temporary Table ###")
spark.sql("SELECT * FROM global_temp.global_customers LIMIT 5").show()

# ---------------- Step 3: Create a Persistent Table (Stored in Hive Metastore) ---------------- #
spark.sql("DROP TABLE IF EXISTS customers_persistent")
df.write.mode("overwrite").saveAsTable("customers_persistent")

print("### Querying Persistent Table (Accessible across sessions and applications) ###")
spark.sql("SELECT * FROM customers_persistent LIMIT 5").show()

