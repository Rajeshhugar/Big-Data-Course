


from pyspark.sql import SparkSession


spark = SparkSession.builder \
.appName('Spark Table Caching')\
.enableHiveSupport()\
.getOrCreate()


spark.sql('show tables').show()


df = spark.read.format('csv').option('header','true').load('/tmp/customers_100.csv')


df.show()


df.write.format('csv').saveAsTable('default.customers_100')


spark.sql('show tables').show()


spark.sql('describe extended customers_100').show(truncate=False)


!hadoop fs -ls /user/hive/warehouse/customers_100


spark.sql('select * from customers_100 limit 5').show()


spark.sql('describe customers_100').show()


spark.sql('select * from customers_100 limit 5').show()


spark.sql('cache table customers_100')





spark.sql('select * from customers_100 limit 5').show()


spark.sql('show tables').show()


spark.sql('describe extended customers_500mb').show(truncate = False)


spark.sql('select * from customers_500mb where city ="Hyderabad" limit 5').show()


spark.sql('cache table customers_500mb')


spark.sql('select * from customers_500mb where city ="Hyderabad" limit 5').show()


spark.sql('uncache table customers_500mb')





spark.sql('cache lazy table customers_500mb')


spark.sql('select * from customers_500mb limit 5').show()


spark.sql('select city, count (*) from customers_500mb group by city').show()


spark.sql('select city, count (*) from customers_500mb group by city').explain(mode='extended')


spark.sql('select city, count (*) from customers_500mb where city ="Hyderabad" group by city').show()


spark.sql('select city, count (*) from customers_500mb where city ="Hyderabad" group by city').explain(mode='extended')


spark.sql('describe extended external_customers_2').show(truncate = False)


spark.sql('cache table external_customers_2').show(truncate = False)


df.show()


spark.stop()
