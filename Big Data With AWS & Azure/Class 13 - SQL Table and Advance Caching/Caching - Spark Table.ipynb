{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618e7a7f-5e96-43ec-8ed5-fcb76fab9b12",
   "metadata": {},
   "source": [
    "# Caching In Spark Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e284c85d-245c-449f-9d20-47012f2042c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374ed9dc-f6c1-484e-bbc2-693aab1aac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName('Spark Table Caching')\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5389358b-9234-40b1-a996-69c58bbbc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6feffcfd-5695-4164-802c-71c89bc02b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header','true').load('/tmp/customers_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c1f855-a7b2-4887-9d55-3f74a1167865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|customer_id|       name|     city|      state|country|registration_date|is_active|\n",
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n",
      "|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n",
      "|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n",
      "|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n",
      "|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n",
      "|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    False|\n",
      "|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    False|\n",
      "|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     True|\n",
      "|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     True|\n",
      "|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     True|\n",
      "|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     True|\n",
      "|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    False|\n",
      "|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    False|\n",
      "|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     True|\n",
      "|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    False|\n",
      "|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     True|\n",
      "|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    False|\n",
      "|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    False|\n",
      "|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     True|\n",
      "|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     True|\n",
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28247ab-19c6-4abd-a831-9d5cbbb02ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('csv').saveAsTable('default.customers_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807354a-e32b-40d8-9303-ca381702e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a1280-670b-4e8e-90f5-0352981339a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('describe extended customers_100').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf255c61-cc3b-4de8-9ca9-aa07eae35345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls /user/hive/warehouse/customers_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a360b-94c7-4e36-8389-e0beb14f24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from customers_100 limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a99528-4c87-4c1b-987b-cbd7a1313bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('describe customers_100').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee030ee-4345-4f88-84f4-3539a3a639fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from customers_100 limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404d798-d039-453c-ba4d-cd71c3e8142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('cache table customers_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23625698-8974-4a31-bfbe-6684444aba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054aca52-629d-4673-b1a5-cadce0651826",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from customers_100 limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16572e41-fddf-4353-9b7b-c617d2c5acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12b34d1-1323-4b9d-a376-169de1d237b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                              |comment|\n",
      "+----------------------------+-------------------------------------------------------+-------+\n",
      "|customers_id                |int                                                    |null   |\n",
      "|name                        |string                                                 |null   |\n",
      "|city                        |string                                                 |null   |\n",
      "|state                       |string                                                 |null   |\n",
      "|country                     |string                                                 |null   |\n",
      "|registration_date           |string                                                 |null   |\n",
      "|is_active                   |boolean                                                |null   |\n",
      "|                            |                                                       |       |\n",
      "|# Detailed Table Information|                                                       |       |\n",
      "|Database                    |default                                                |       |\n",
      "|Table                       |customers_500mb                                        |       |\n",
      "|Owner                       |root                                                   |       |\n",
      "|Created Time                |Thu Feb 06 11:40:32 UTC 2025                           |       |\n",
      "|Last Access                 |UNKNOWN                                                |       |\n",
      "|Created By                  |Spark 3.3.2                                            |       |\n",
      "|Type                        |MANAGED                                                |       |\n",
      "|Provider                    |csv                                                    |       |\n",
      "|Statistics                  |570783941 bytes                                        |       |\n",
      "|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers_500mb|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     |       |\n",
      "+----------------------------+-------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended customers_500mb').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d4d10c-de21-4364-bd5f-16ff4ecf4194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================>                                   (2 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|customers_id|       name|     city|      state|country|registration_date|is_active|\n",
      "+------------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|           3| Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n",
      "|           6| Customer_6|Hyderabad| Tamil Nadu|  India|       2023-07-17|    false|\n",
      "|           7| Customer_7|Hyderabad| Tamil Nadu|  India|       2023-08-18|     true|\n",
      "|          20|Customer_20|Hyderabad| Tamil Nadu|  India|       2023-02-19|     true|\n",
      "|          26|Customer_26|Hyderabad|Maharashtra|  India|       2023-12-13|     true|\n",
      "+------------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from customers_500mb where city =\"Hyderabad\" limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76fe24fd-2864-4fe4-a3bb-704caedd1792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('cache table customers_500mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45adb093-12b3-46bd-8fbf-2c4b4880fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|customers_id|       name|     city|      state|country|registration_date|is_active|\n",
      "+------------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|           3| Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n",
      "|           6| Customer_6|Hyderabad| Tamil Nadu|  India|       2023-07-17|    false|\n",
      "|           7| Customer_7|Hyderabad| Tamil Nadu|  India|       2023-08-18|     true|\n",
      "|          20|Customer_20|Hyderabad| Tamil Nadu|  India|       2023-02-19|     true|\n",
      "|          26|Customer_26|Hyderabad|Maharashtra|  India|       2023-12-13|     true|\n",
      "+------------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from customers_500mb where city =\"Hyderabad\" limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3f0440-656e-4ad0-a0f2-6c8fe54c750d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('uncache table customers_500mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc19d60-68dc-4ccb-895c-0bbcd279c6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c026cfe-e333-4332-ac66-4229fe97479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('cache lazy table customers_500mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6af04dd-1e9f-4f8c-8521-86f84fe0cdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|customers_id|      name|     city|      state|country|registration_date|is_active|\n",
      "+------------+----------+---------+-----------+-------+-----------------+---------+\n",
      "|        null|      name|     city|      state|country|registration_date|     null|\n",
      "|           0|Customer_0|   Mumbai|  Telangana|  India|       2023-03-21|     true|\n",
      "|           1|Customer_1|  Chennai|West Bengal|  India|       2023-05-27|    false|\n",
      "|           2|Customer_2|     Pune|  Karnataka|  India|       2023-10-11|    false|\n",
      "|           3|Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n",
      "+------------+----------+---------+-----------+-------+-----------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from customers_500mb limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8217c7f5-63be-42cc-ac70-728407917599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|     city|count(1)|\n",
      "+---------+--------+\n",
      "|Bangalore| 1094195|\n",
      "|  Chennai| 1095052|\n",
      "|   Mumbai| 1095815|\n",
      "|Ahmedabad| 1097162|\n",
      "|  Kolkata| 1096777|\n",
      "|     city|       1|\n",
      "|     Pune| 1095748|\n",
      "|    Delhi| 1096183|\n",
      "|Hyderabad| 1096426|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select city, count (*) from customers_500mb group by city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc7ab8c-574f-4e9c-9a69-bfe237c4446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, unresolvedalias('count(1), None)]\n",
      "+- 'UnresolvedRelation [customers_500mb], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, count(1): bigint\n",
      "Aggregate [city#151], [city#151, count(1) AS count(1)#1202L]\n",
      "+- SubqueryAlias spark_catalog.default.customers_500mb\n",
      "   +- Relation default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#151], [city#151, count(1) AS count(1)#1202L]\n",
      "+- Project [city#151]\n",
      "   +- InMemoryRelation [customers_id#149, name#150, city#151, state#152, country#153, registration_date#154, is_active#155], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- FileScan csv default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://my-cluster-m/user/hive/warehouse/customers_500mb], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customers_id:int,name:string,city:string,state:string,country:string,registration_date:str...\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#151], functions=[count(1)], output=[city#151, count(1)#1202L])\n",
      "   +- Exchange hashpartitioning(city#151, 200), ENSURE_REQUIREMENTS, [plan_id=325]\n",
      "      +- HashAggregate(keys=[city#151], functions=[partial_count(1)], output=[city#151, count#1311L])\n",
      "         +- Scan In-memory table customers_500mb [city#151]\n",
      "               +- InMemoryRelation [customers_id#149, name#150, city#151, state#152, country#153, registration_date#154, is_active#155], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- FileScan csv default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://my-cluster-m/user/hive/warehouse/customers_500mb], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customers_id:int,name:string,city:string,state:string,country:string,registration_date:str...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select city, count (*) from customers_500mb group by city').explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cc6d1fb-3e43-4c95-96df-d38297f414cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|     city|count(1)|\n",
      "+---------+--------+\n",
      "|Hyderabad| 1096426|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select city, count (*) from customers_500mb where city =\"Hyderabad\" group by city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c1771a6-a954-4b82-a37e-c9fa5b02eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city], ['city, unresolvedalias('count(1), None)]\n",
      "+- 'Filter ('city = Hyderabad)\n",
      "   +- 'UnresolvedRelation [customers_500mb], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, count(1): bigint\n",
      "Aggregate [city#151], [city#151, count(1) AS count(1)#1614L]\n",
      "+- Filter (city#151 = Hyderabad)\n",
      "   +- SubqueryAlias spark_catalog.default.customers_500mb\n",
      "      +- Relation default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#151], [city#151, count(1) AS count(1)#1614L]\n",
      "+- Project [city#151]\n",
      "   +- Filter (isnotnull(city#151) AND (city#151 = Hyderabad))\n",
      "      +- InMemoryRelation [customers_id#149, name#150, city#151, state#152, country#153, registration_date#154, is_active#155], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- FileScan csv default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://my-cluster-m/user/hive/warehouse/customers_500mb], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customers_id:int,name:string,city:string,state:string,country:string,registration_date:str...\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[city#151], functions=[count(1)], output=[city#151, count(1)#1614L])\n",
      "   +- Exchange hashpartitioning(city#151, 200), ENSURE_REQUIREMENTS, [plan_id=403]\n",
      "      +- HashAggregate(keys=[city#151], functions=[partial_count(1)], output=[city#151, count#1723L])\n",
      "         +- Filter (isnotnull(city#151) AND (city#151 = Hyderabad))\n",
      "            +- Scan In-memory table customers_500mb [city#151], [isnotnull(city#151), (city#151 = Hyderabad)]\n",
      "                  +- InMemoryRelation [customers_id#149, name#150, city#151, state#152, country#153, registration_date#154, is_active#155], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- FileScan csv default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://my-cluster-m/user/hive/warehouse/customers_500mb], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customers_id:int,name:string,city:string,state:string,country:string,registration_date:str...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select city, count (*) from customers_500mb where city =\"Hyderabad\" group by city').explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74380b1a-5e68-4779-8f49-3e32496a0f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                         |comment|\n",
      "+----------------------------+--------------------------------------------------+-------+\n",
      "|customer_id                 |int                                               |null   |\n",
      "|name                        |string                                            |null   |\n",
      "|city                        |string                                            |null   |\n",
      "|state                       |string                                            |null   |\n",
      "|country                     |string                                            |null   |\n",
      "|registration_date           |string                                            |null   |\n",
      "|is_active                   |boolean                                           |null   |\n",
      "|                            |                                                  |       |\n",
      "|# Detailed Table Information|                                                  |       |\n",
      "|Database                    |default                                           |       |\n",
      "|Table                       |external_customers_2                              |       |\n",
      "|Owner                       |root                                              |       |\n",
      "|Created Time                |Wed Feb 05 11:55:14 UTC 2025                      |       |\n",
      "|Last Access                 |UNKNOWN                                           |       |\n",
      "|Created By                  |Spark 3.3.2                                       |       |\n",
      "|Type                        |EXTERNAL                                          |       |\n",
      "|Provider                    |csv                                               |       |\n",
      "|Location                    |hdfs://my-cluster-m/data/external_data            |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat  |       |\n",
      "+----------------------------+--------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended external_customers_2').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f9bf376-e2d6-48da-b553-80e1f22c3fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('cache table external_customers_2').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34f136ff-4af7-44d0-82b4-359849117933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|customer_id|       name|     city|      state|country|registration_date|is_active|\n",
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n",
      "|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n",
      "|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n",
      "|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n",
      "|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n",
      "|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    False|\n",
      "|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    False|\n",
      "|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     True|\n",
      "|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     True|\n",
      "|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     True|\n",
      "|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     True|\n",
      "|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    False|\n",
      "|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    False|\n",
      "|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     True|\n",
      "|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    False|\n",
      "|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     True|\n",
      "|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    False|\n",
      "|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    False|\n",
      "|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     True|\n",
      "|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     True|\n",
      "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e906142-7f89-4f9c-8e08-bd9bec790292",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
