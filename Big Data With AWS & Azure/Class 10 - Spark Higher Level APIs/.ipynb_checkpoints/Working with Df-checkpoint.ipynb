{"cells": [{"cell_type": "code", "execution_count": 1, "id": "0b1cb37a-676e-4d5b-bc0a-cb6b0eccf455", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/01/25 04:44:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\n# Initialize Spark Session\n# Initialize Spark Session with dynamic allocation\nspark = SparkSession.builder \\\n.appName(\"Higher_Level_APIs\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "5010fca1-15b8-420c-8efc-020c7971ab8f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 7 items\n-rw-r--r--   2 root       hadoop    327.4 M 2025-01-20 10:42 /tmp/customers.csv\n-rw-r--r--   2 root       hadoop     10.0 M 2025-01-20 10:31 /tmp/customers_10mb.csv\n-rw-r--r--   2 mayank0953 hadoop      1.0 M 2025-01-20 10:23 /tmp/customers_1mb.csv\n-rw-r--r--   2 mayank0953 hadoop    544.3 M 2025-01-22 17:12 /tmp/customers_500mb.csv\ndrwxr-xr-x   - anonymous  hadoop          0 2025-01-23 16:41 /tmp/external\ndrwxrwxrwt   - hdfs       hadoop          0 2025-01-20 09:57 /tmp/hadoop-yarn\ndrwx-wx-wx   - hive       hadoop          0 2025-01-20 09:57 /tmp/hive\n"}], "source": "!hadoop fs -ls -h /tmp/"}, {"cell_type": "code", "execution_count": null, "id": "8c8d20e9-3863-4ef4-9229-2d2da9972cc8", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 5, "id": "b3d6c751-f009-46db-ab7c-45665d1a06fc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read \\\n    .format('csv') \\\n    .option('header', 'true') \\\n    .option('inferSchema', 'true') \\\n    .load('/tmp/customers_1mb.csv')\n"}, {"cell_type": "code", "execution_count": null, "id": "fe499998-8a4d-4972-a7fd-26a13b3b2b28", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "dcdc0d6d-0449-4f5c-9892-5bcbe291d56e", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 7, "id": "074cca89-0d9b-4160-994b-c05747fe121b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-------------------+---------+\n|customer_id|      name|     city|      state|country|  registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\nonly showing top 5 rows\n\n"}], "source": "df.show(5)"}, {"cell_type": "code", "execution_count": 10, "id": "0bdeaf0f-18aa-4c24-af55-465cee41f2dc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n |-- registration_date: timestamp (nullable = true)\n |-- is_active: boolean (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": 11, "id": "14119f8f-912c-4e30-b4de-e0fc4377c37a", "metadata": {}, "outputs": [], "source": "df.createOrReplaceTempView('customers')"}, {"cell_type": "code", "execution_count": 13, "id": "3be1f471-ff7a-45e6-be51-4cb4065ddc9f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-------------------+---------+\n|customer_id|      name|     city|      state|country|  registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n\n"}], "source": "spark.sql('''select * from customers \nlimit 5''').show()"}, {"cell_type": "code", "execution_count": 16, "id": "d8175289-404f-4ad7-8a23-0a5089007837", "metadata": {}, "outputs": [{"ename": "AnalysisException", "evalue": "Column 'citi' does not exist. Did you mean one of the following? [customers.city, customers.name, customers.state, customers.country, customers.is_active, customers.customer_id, customers.registration_date]; line 1 pos 19;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project [customer_id#48, 'citi]\n      +- SubqueryAlias customers\n         +- View (`customers`, [customer_id#48,name#49,city#50,state#51,country#52,registration_date#53,is_active#54])\n            +- Relation [customer_id#48,name#49,city#50,state#51,country#52,registration_date#53,is_active#54] csv\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_25579/2399671434.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select customer_id,citi from customers limit 5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: Column 'citi' does not exist. Did you mean one of the following? [customers.city, customers.name, customers.state, customers.country, customers.is_active, customers.customer_id, customers.registration_date]; line 1 pos 19;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project [customer_id#48, 'citi]\n      +- SubqueryAlias customers\n         +- View (`customers`, [customer_id#48,name#49,city#50,state#51,country#52,registration_date#53,is_active#54])\n            +- Relation [customer_id#48,name#49,city#50,state#51,country#52,registration_date#53,is_active#54] csv\n"]}], "source": "spark.sql('select customer_id,citi from customers limit 5')"}, {"cell_type": "code", "execution_count": 17, "id": "09dd47ac-249c-41aa-bd0e-ee4c3abaa38a", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "5531da04-be0d-4040-93c3-d268262df0b3", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 18, "id": "6c6eed3f-14d6-4791-a80f-2d496d2d0048", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/01/25 05:05:57 INFO SparkEnv: Registering MapOutputTracker\n25/01/25 05:05:57 INFO SparkEnv: Registering BlockManagerMaster\n25/01/25 05:05:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/01/25 05:05:57 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\n# Initialize Spark Session\n# Initialize Spark Session with dynamic allocation\nspark = SparkSession.builder \\\n.appName(\"Higher_Level_APIs_2\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": null, "id": "263e0cdc-413e-4efe-80ac-b96fd447ee78", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 19, "id": "08f38a28-8e11-4434-bba3-97074f904725", "metadata": {}, "outputs": [], "source": "rdd = spark.sparkContext.parallelize([\n    ('0','Customer_0','Banagalore','India'),\n    ('1','Customer_1','Delhi','India')])"}, {"cell_type": "code", "execution_count": 22, "id": "d045684d-f547-4d15-bf76-aaf46d3d7fe8", "metadata": {}, "outputs": [], "source": "df = spark.createDataFrame(rdd,['cust_id','cust_name','state','country'])"}, {"cell_type": "code", "execution_count": 23, "id": "fec79246-607f-41d0-aa39-6d5da15f0ef1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+----------+----------+-------+\n|cust_id| cust_name|     state|country|\n+-------+----------+----------+-------+\n|      0|Customer_0|Banagalore|  India|\n|      1|Customer_1|     Delhi|  India|\n+-------+----------+----------+-------+\n\n"}], "source": "df.show()"}, {"cell_type": "code", "execution_count": 24, "id": "8a557d1c-7b70-41b3-9a6b-051f69de1a87", "metadata": {}, "outputs": [], "source": "df.createOrReplaceTempView('customers')"}, {"cell_type": "code", "execution_count": 26, "id": "cc4b2dc5-a7a9-407f-8a23-1e7b3812309b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+\n|cust_id|\n+-------+\n|      0|\n|      1|\n+-------+\n\n"}], "source": "spark.sql('select cust_id from customers').show()"}, {"cell_type": "code", "execution_count": 28, "id": "a1a742aa-0bd7-419a-b8b0-9a67dca9d609", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- cust_id: string (nullable = true)\n |-- cust_name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": 30, "id": "f097dda6-69ef-4e1e-908b-6ee2a751f8a5", "metadata": {}, "outputs": [], "source": "selected_cols = df.select('cust_id','cust_name')"}, {"cell_type": "code", "execution_count": 31, "id": "07023d0e-a239-4c54-8961-85708d24bd01", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+----------+\n|cust_id| cust_name|\n+-------+----------+\n|      0|Customer_0|\n|      1|Customer_1|\n+-------+----------+\n\n"}], "source": "selected_cols.show()"}, {"cell_type": "code", "execution_count": 32, "id": "98ca5dc2-48f9-4ae9-9bbf-ff95abc5869a", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "4ef87306-1f62-4c0e-bd88-d60a285591c6", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "ef0dbfa4-ff91-4790-be94-344626a5258b", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "09288cac-e0d0-4f2d-ade2-4771994aa930", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "a329350b-d141-463d-a458-141f0771d96f", "metadata": {}, "source": "# Read  Dataframe "}, {"cell_type": "code", "execution_count": 38, "id": "7a7df07b-9da4-4274-a4df-28b0f779018a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/01/25 05:23:41 INFO SparkEnv: Registering MapOutputTracker\n25/01/25 05:23:41 INFO SparkEnv: Registering BlockManagerMaster\n25/01/25 05:23:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/01/25 05:23:41 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\n# Initialize Spark Session\n# Initialize Spark Session with dynamic allocation\nspark = SparkSession.builder \\\n.appName(\"Higher_Level_APIs_3\") \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 39, "id": "31b3b9ab-cdf1-477a-879d-69bfa18619bc", "metadata": {}, "outputs": [], "source": "hdfs_path = '/tmp/customers_500mb.csv'\n"}, {"cell_type": "code", "execution_count": 40, "id": "404b0218-8ad3-4c6e-80d0-39a23205276f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read \\\n    .format('csv') \\\n    .load(hdfs_path)"}, {"cell_type": "code", "execution_count": 41, "id": "8230b7a5-08ff-4505-b39d-a35a421ff9e2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read \\\n    .format('csv') \\\n    .option('inferSchema', 'true') \\\n    .option('header', 'true') \\\n    .load(hdfs_path)"}, {"cell_type": "code", "execution_count": 42, "id": "409ba4e8-1622-4e5b-b2bd-4b6d226bf44f", "metadata": {}, "outputs": [], "source": "df_header_only = spark.read \\\n    .format('csv') \\\n    .option('header', 'true') \\\n    .load(hdfs_path)"}, {"cell_type": "code", "execution_count": 58, "id": "49e4f107-d82a-4bd9-ab5f-51c2b6c417a4", "metadata": {}, "outputs": [], "source": "from pyspark.sql.types import *\n\nschema = StructType([\nStructField(\"customer_id\", IntegerType(), True),\nStructField(\"name_of_customer\", StringType(), True),\nStructField(\"city\", StringType(), True),\nStructField(\"state\", StringType(), True),\nStructField(\"country\", StringType(), True),\nStructField(\"registration_date\", StringType(), True),\nStructField(\"is_active\", BooleanType(), True),\n])\n"}, {"cell_type": "code", "execution_count": 61, "id": "86a8d57c-bc9a-414a-9d07-3d85e3d00bfc", "metadata": {}, "outputs": [], "source": "df_explicit = spark.read \\\n.format(\"csv\") \\\n.option(\"header\", \"false\") \\\n.schema(schema) \\\n.load (hdfs_path)\n"}, {"cell_type": "code", "execution_count": 62, "id": "502b1fe0-c5f6-4e14-95de-505e9eae47dd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------------+---------+-----------+-------+-----------------+---------+\n|customer_id|name_of_customer|     city|      state|country|registration_date|is_active|\n+-----------+----------------+---------+-----------+-------+-----------------+---------+\n|       null|            name|     city|      state|country|registration_date|     null|\n|          0|      Customer_0|   Mumbai|  Telangana|  India|       2023-03-21|     true|\n|          1|      Customer_1|  Chennai|West Bengal|  India|       2023-05-27|    false|\n|          2|      Customer_2|     Pune|  Karnataka|  India|       2023-10-11|    false|\n|          3|      Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n+-----------+----------------+---------+-----------+-------+-----------------+---------+\nonly showing top 5 rows\n\n"}], "source": "df_explicit.show(5)"}, {"cell_type": "code", "execution_count": 48, "id": "63893a00-3305-44d7-91cb-a659ece0c312", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_4 = spark.read \\\n    .format('csv') \\\n    .option('inferSchema', 'true') \\\n    .option('header', 'true') \\\n    .load(hdfs_path)"}, {"cell_type": "code", "execution_count": 53, "id": "b708b3d2-6763-4085-bf4a-77026e7decae", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_5 = spark.read \\\n    .format('csv') \\\n    .option('inferSchema', 'true') \\\n    .option('header', 'true') \\\n    .option('samplingRatio',0.1)\\\n    .load(hdfs_path)"}, {"cell_type": "code", "execution_count": null, "id": "e5446e69-7d55-4e5b-97d2-993706596746", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "fe3b8579-1d3e-4173-8df7-454ee5e43166", "metadata": {}, "outputs": [], "source": "from pyspark.sql.types import *\n\nschema = StructType([\nStructField(\"customer_id\", IntegerType(), True),\nStructField(\"name_of_customer\", StringType(), True),\nStructField(\"city\", StringType(), True),\nStructField(\"state\", StringType(), True),\nStructField(\"country\", StringType(), True),\nStructField(\"registration_date\", StringType(), True),\nStructField(\"is_active\", BooleanType(), True),\n])\n"}, {"cell_type": "code", "execution_count": 90, "id": "52aa11e7-0faa-4c42-bfbc-b473419407ab", "metadata": {}, "outputs": [], "source": "ddl_schema = \" customer_id INT NOT NULL, name INT, city STRING, state STRING,country STRING, registration_date TIMESTAMP,is_active BOOLEAN\""}, {"cell_type": "code", "execution_count": 91, "id": "e77faa60-b78f-4a7f-b07d-09e612009305", "metadata": {}, "outputs": [], "source": "df_ddl_explicit = spark.read \\\n.format(\"csv\") \\\n.option(\"header\", \"true\") \\\n.schema(ddl_schema) \\\n.load (hdfs_path)\n"}, {"cell_type": "code", "execution_count": 92, "id": "be47de08-1ac8-4cf3-9a20-58836efd9fc1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----+---------+-----------+-------+-------------------+---------+\n|customer_id|name|     city|      state|country|  registration_date|is_active|\n+-----------+----+---------+-----------+-------+-------------------+---------+\n|          0|null|   Mumbai|  Telangana|  India|2023-03-21 00:00:00|     true|\n|          1|null|  Chennai|West Bengal|  India|2023-05-27 00:00:00|    false|\n|          2|null|     Pune|  Karnataka|  India|2023-10-11 00:00:00|    false|\n|          3|null|Hyderabad|    Gujarat|  India|2023-11-11 00:00:00|    false|\n|          4|null|   Mumbai|  Karnataka|  India|2023-05-09 00:00:00|    false|\n+-----------+----+---------+-----------+-------+-------------------+---------+\nonly showing top 5 rows\n\n"}], "source": "df_ddl_explicit.show(5)"}, {"cell_type": "code", "execution_count": 93, "id": "d315cae4-dccc-46ff-8605-f0b2884399db", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: integer (nullable = true)\n |-- name: integer (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n |-- registration_date: timestamp (nullable = true)\n |-- is_active: boolean (nullable = true)\n\n"}], "source": "df_ddl_explicit.printSchema()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}