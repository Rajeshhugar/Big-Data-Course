{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Operations in PySpark\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to DataFrame Operations](#1-introduction-to-dataframe-operations)\n",
    "2. [Loading Data](#2-loading-data)\n",
    "3. [Selecting Columns](#3-selecting-columns)\n",
    "4. [Filtering Rows](#4-filtering-rows)\n",
    "5. [Adding/Renaming/Dropping Columns](#5-adding-renaming-dropping-columns)\n",
    "6. [Sorting Data](#6-sorting-data)\n",
    "7. [Handling Missing Data](#7-handling-missing-data)\n",
    "8. [Distinct and Duplicate Handling](#8-distinct-and-duplicate-handling)\n",
    "9. [Union Operations](#9-union-operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to DataFrame Operations\n",
    "\n",
    "DataFrame operations are the backbone of data manipulation in PySpark. They allow you to transform, filter, and analyze data efficiently. In this notebook, we’ll explore various DataFrame operations using the **customer dataset** you provided.\n",
    "\n",
    "### Key Concepts\n",
    "- **DataFrame**: A distributed collection of data organized into named columns.\n",
    "- **Transformations**: Operations that produce a new DataFrame (e.g., `select`, `filter`).\n",
    "- **Actions**: Operations that trigger computation and return results (e.g., `show`, `count`).\n",
    "\n",
    "```\n",
    "┌──────────────┐\n",
    "│  DataFrame   │\n",
    "└──────┬───────┘\n",
    "       │\n",
    "       ▼\n",
    "┌───────────────────┐\n",
    "│  Transformations  │\n",
    "│  (e.g., select,   │\n",
    "│   filter, groupBy)│\n",
    "└─────────┬─────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌───────────────────┐\n",
    "│  Actions          │\n",
    "│  (e.g., show,     │\n",
    "│   count, collect) │\n",
    "└───────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "\n",
    "Before performing any operations, we need to load the customer dataset into a DataFrame.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
    "\n",
    "# Load the customer dataset\n",
    "df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "| customer_id | name        | city      | state        | country | registration_date | is_active |\n",
    "|-------------|-------------|-----------|--------------|---------|-------------------|-----------|\n",
    "| 0           | Customer_0  | Pune      | West Bengal  | India   | 2023-10-10        | True      |\n",
    "| 1           | Customer_1  | Bangalore | Gujarat      | India   | 2023-10-19        | False     |\n",
    "| 2           | Customer_2  | Bangalore | Karnataka    | India   | 2023-02-10        | True      |\n",
    "| 3           | Customer_3  | Bangalore | Telangana    | India   | 2023-03-24        | True      |\n",
    "| 4           | Customer_4  | Hyderabad | Telangana    | India   | 2023-06-04        | False     |\n",
    "\n",
    "### Explanation\n",
    "- **`spark.read.csv`**: Loads a CSV file into a DataFrame.\n",
    "- **`header=True`**: Uses the first row as column names.\n",
    "- **`inferSchema=True`**: Automatically infers the data types of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting Columns\n",
    "\n",
    "Selecting specific columns from a DataFrame is a common operation. You can use the `select` method to choose columns.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "# Select specific columns\n",
    "df.select(\"customer_id\", \"name\", \"city\").show(5)\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "| customer_id | name        | city      |\n",
    "|-------------|-------------|-----------|\n",
    "| 0           | Customer_0  | Pune      |\n",
    "| 1           | Customer_1  | Bangalore |\n",
    "| 2           | Customer_2  | Bangalore |\n",
    "| 3           | Customer_3  | Bangalore |\n",
    "| 4           | Customer_4  | Hyderabad |\n",
    "\n",
    "### Explanation\n",
    "- **`select`**: Selects specific columns from the DataFrame.\n",
    "- **`show(5)`**: Displays the first 5 rows of the selected columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering Rows\n",
    "\n",
    "Filtering rows based on conditions is essential for data analysis. You can use the `filter` or `where` method to filter rows.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "# Filter rows where city is 'Bangalore'\n",
    "df.filter(df[\"city\"] == \"Bangalore\").show(5)\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "| customer_id | name        | city      | state      | country | registration_date | is_active |\n",
    "|-------------|-------------|-----------|------------|---------|-------------------|-----------|\n",
    "| 1           | Customer_1  | Bangalore | Gujarat    | India   | 2023-10-19        | False     |\n",
    "| 2           | Customer_2  | Bangalore | Karnataka  | India   | 2023-02-10        | True      |\n",
    "| 3           | Customer_3  | Bangalore | Telangana  | India   | 2023-03-24        | True      |\n",
    "| 7           | Customer_7  | Bangalore | Telangana  | India   | 2023-08-25        | True      |\n",
    "| 8           | Customer_8  | Bangalore | Maharashtra| India   | 2023-07-13        | False     |\n",
    "\n",
    "### Explanation\n",
    "- **`filter`**: Filters rows based on a condition.\n",
    "- **`df[\"city\"] == \"Bangalore\"`**: Condition to filter rows where the city is Bangalore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding/Renaming/Dropping Columns\n",
    "\n",
    "You can add, rename, or drop columns in a DataFrame using methods like `withColumn`, `withColumnRenamed`, and `drop`.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Add a new column 'is_premium' with a default value of False\n",
    "df = df.withColumn(\"is_premium\", lit(False))\n",
    "\n",
    "# Rename the column 'is_active' to 'active_status'\n",
    "df = df.withColumnRenamed(\"is_active\", \"active_status\")\n",
    "\n",
    "# Drop the column 'state'\n",
    "df = df.drop(\"state\")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "| customer_id | name        | city      | country | registration_date | active_status | is_premium |\n",
    "|-------------|-------------|-----------|---------|-------------------|---------------|------------|\n",
    "| 0           | Customer_0  | Pune      | India   | 2023-10-10        | True          | False      |\n",
    "| 1           | Customer_1  | Bangalore | India   | 2023-10-19        | False         | False      |\n",
    "| 2           | Customer_2  | Bangalore | India   | 2023-02-10        | True          | False      |\n",
    "| 3           | Customer_3  | Bangalore | India   | 2023-03-24        | True          | False      |\n",
    "| 4           | Customer_4  | Hyderabad | India   | 2023-06-04        | False         | False      |\n",
    "\n",
    "### Explanation\n",
    "- **`withColumn`**: Adds a new column or updates an existing one.\n",
    "- **`withColumnRenamed`**: Renames a column.\n",
    "- **`drop`**: Drops a column from the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sorting Data\n",
    "\n",
    "Sorting data is useful for organizing and analyzing data. You can use the `orderBy` or `sort` method to sort rows.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "# Sort by 'registration_date' in descending order\n",
    "df.orderBy(\"registration_date\", ascending=False).show(5)\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "| customer_id | name        | city      | country | registration_date | active_status | is_premium |\n",
    "|-------------|-------------|-----------|---------|-------------------|---------------|------------|\n",
    "| 1           | Customer_1  | Bangalore | India   | 2023-10-19        | False         | False      |\n",
    "| 0           | Customer_0  | Pune      | India   | 2023-10-10        | True          | False      |\n",
    "| 7           | Customer_7  | Bangalore | India   | 2023-08-25        | True          | False      |\n",
    "| 8           | Customer_8  | Bangalore | India   | 2023-07-13        | False         | False      |\n",
    "| 5           | Customer_5  | Hyderabad | India   | 2023-07-26        | True          | False      |\n",
    "\n",
    "### Explanation\n",
    "- **`orderBy`**: Sorts the DataFrame by specified columns.\n",
    "- **`ascending=False`**: Sorts in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Missing Data\n",
    "\n",
    "Handling missing data is crucial for accurate analysis. You can use methods like `na.fill` and `na.drop` to handle missing values.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "# Fill missing values in 'city' with 'Unknown'\n",
    "df.na.fill({\"city\": \"Unknown\"}).show(5)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.na.drop().show(5)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "- **`na.fill`**: Fills missing values with specified values.\n",
    "- **`na.drop`**: Drops rows with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Distinct and Duplicate Handling\n",
    "\n",
    "You can remove duplicate rows using the `distinct` and `dropDuplicates` methods.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "# Remove duplicate rows\n",
    "df.distinct().show(5)\n",
    "\n",
    "# Remove duplicates based on specific columns\n",
    "df.dropDuplicates([\"city\", \"state\"]).show(5)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "- **`distinct`**: Removes duplicate rows from the entire DataFrame.\n",
    "- **`dropDuplicates`**: Removes duplicates based on specific columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Union Operations\n",
    "\n",
    "You can combine two DataFrames using the `union` method.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "# Create another DataFrame with similar schema\n",
    "new_data = [\n",
    "    (10, \"Customer_10\", \"Mumbai\", \"Maharashtra\", \"India\", \"2023-09-01\", True),\n",
    "    (11, \"Customer_11\", \"Chennai\", \"Tamil Nadu\", \"India\", \"2023-09-15\", False)\n",
    "]\n",
    "new_df = spark.createDataFrame(new_data, schema=df.schema)\n",
    "\n",
    "# Union the two DataFrames\n",
    "combined_df = df.union(new_df)\n",
    "combined_df.show()\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "- **`union`**: Combines two DataFrames with the same schema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python",
   "display_name": "Python (Pyodide)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4
}