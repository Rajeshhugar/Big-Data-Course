{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations in PySpark\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Aggregations](#1-introduction-to-aggregations)\n",
    "2. [Simple Aggregations](#2-simple-aggregations)\n",
    "3. [Grouped Aggregations](#3-grouped-aggregations)\n",
    "4. [Window Functions](#4-window-functions)\n",
    "5. [Cumulative Aggregations](#5-cumulative-aggregations)\n",
    "6. [Pivoting Data](#6-pivoting-data)\n",
    "7. [Summary](#7-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Aggregations\n",
    "\n",
    "Aggregations are one of the most common operations in data processing. They allow us to summarize data by computing metrics such as counts, sums, averages, and more. In PySpark, aggregations are essential for transforming large datasets into meaningful insights.\n",
    "\n",
    "### Why are Aggregations Important?\n",
    "- **Data Summarization**: Reduce large datasets into meaningful summaries.\n",
    "- **Insight Generation**: Compute metrics like averages, totals, and counts.\n",
    "- **Data Transformation**: Prepare data for further analysis or visualization.\n",
    "\n",
    "### Key Concepts in Aggregations\n",
    "- **Simple Aggregations**: Basic operations like count, sum, avg, min, and max.\n",
    "- **Grouped Aggregations**: Aggregations performed on grouped data using `groupBy`.\n",
    "- **Window Functions**: Advanced aggregations that operate on a sliding window of rows.\n",
    "- **Cumulative Aggregations**: Aggregations that accumulate over rows, such as running totals.\n",
    "- **Pivoting Data**: Transforming data from long to wide format for better readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Aggregations\n",
    "\n",
    "Simple aggregations are the most basic form of data summarization. They include operations like counting rows, summing values, and computing averages.\n",
    "\n",
    "### Example: Simple Aggregations on Customer Data\n",
    "\n",
    "Let's start by loading the customer data and performing some simple aggregations.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, sum, avg, min, max\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Aggregations\").getOrCreate()\n",
    "\n",
    "# Load customer data\n",
    "df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| customer_id | name       | city      | state       | country | registration_date | is_active |\n",
    "|-------------|------------|-----------|-------------|---------|-------------------|-----------|\n",
    "| 0           | Customer_0 | Pune      | West Bengal | India   | 2023-10-10        | True      |\n",
    "| 1           | Customer_1 | Bangalore | Gujarat     | India   | 2023-10-19        | False     |\n",
    "| 2           | Customer_2 | Bangalore | Karnataka   | India   | 2023-02-10        | True      |\n",
    "| 3           | Customer_3 | Bangalore | Telangana   | India   | 2023-03-24        | True      |\n",
    "| 4           | Customer_4 | Hyderabad | Telangana   | India   | 2023-06-04        | False     |\n",
    "| 5           | Customer_5 | Hyderabad | Telangana   | India   | 2023-06-04        | False     |\n",
    ".\n",
    ".\n",
    "| 10           | Customer_10 | Hyderabad | Telangana   | India   | 2023-06-04        | False     |\n",
    "\n",
    "### Count\n",
    "Count the total number of customers.\n",
    "\n",
    "```python\n",
    "df.select(count(\"*\")).show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| count(1) |\n",
    "|----------|\n",
    "| 11       |\n",
    "\n",
    "### Sum\n",
    "Sum the `customer_id` column (though this may not make sense in this context, it's just an example).\n",
    "\n",
    "```python\n",
    "df.select(sum(\"customer_id\")).show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| sum(customer_id) |\n",
    "|------------------|\n",
    "| 55               |\n",
    "\n",
    "### Average\n",
    "Compute the average `customer_id`.\n",
    "\n",
    "```python\n",
    "df.select(avg(\"customer_id\")).show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| avg(customer_id) |\n",
    "|------------------|\n",
    "| 5.0              |\n",
    "\n",
    "### Min and Max\n",
    "Find the minimum and maximum `customer_id`.\n",
    "\n",
    "```python\n",
    "df.select(min(\"customer_id\"), max(\"customer_id\")).show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| min(customer_id) | max(customer_id) |\n",
    "|------------------|------------------|\n",
    "| 0                | 10               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grouped Aggregations\n",
    "\n",
    "Grouped aggregations allow us to perform aggregations on subsets of data, grouped by one or more columns. This is done using the `groupBy` function.\n",
    "\n",
    "### Example: Grouped Aggregations on Customer Data\n",
    "\n",
    "Let's group the customers by `city` and compute the count, average `customer_id`, and the number of active customers.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import count, avg, sum as _sum\n",
    "\n",
    "# Group by city and perform aggregations\n",
    "grouped_df = df.groupBy(\"city\").agg(\n",
    "    count(\"*\").alias(\"total_customers\"),\n",
    "    avg(\"customer_id\").alias(\"avg_customer_id\"),\n",
    "    _sum(\"is_active\").alias(\"active_customers\")\n",
    ")\n",
    "\n",
    "grouped_df.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| city      | total_customers | avg_customer_id | active_customers |\n",
    "|-----------|-----------------|-----------------|------------------|\n",
    "| Pune      | 1               | 0.0             | 1                |\n",
    "| Bangalore | 5               | 4.0             | 3                |\n",
    "| Hyderabad | 3               | 5.0             | 1                |\n",
    "| Ahmedabad | 1               | 9.0             | 0                |\n",
    "| Chennai   | 1               | 10.0            | 0                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Window Functions\n",
    "\n",
    "Window functions allow us to perform calculations across a set of rows that are related to the current row. This is useful for tasks like ranking, cumulative sums, and moving averages.\n",
    "\n",
    "### Example: Window Functions on Customer Data\n",
    "\n",
    "Let's rank customers within each city based on their `customer_id`.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank\n",
    "\n",
    "# Define a window partitioned by city and ordered by customer_id\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(\"customer_id\")\n",
    "\n",
    "# Add row number, rank, and dense rank\n",
    "df_with_rank = df.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "                 .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "                 .withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "\n",
    "df_with_rank.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| customer_id | name       | city      | state       | country | registration_date | is_active | row_number | rank | dense_rank |\n",
    "|-------------|------------|-----------|-------------|---------|-------------------|-----------|------------|------|------------|\n",
    "| 0           | Customer_0 | Pune      | West Bengal | India   | 2023-10-10        | True      | 1          | 1    | 1          |\n",
    "| 1           | Customer_1 | Bangalore | Gujarat     | India   | 2023-10-19        | False     | 1          | 1    | 1          |\n",
    "| 2           | Customer_2 | Bangalore | Karnataka   | India   | 2023-02-10        | True      | 2          | 2    | 2          |\n",
    "| 3           | Customer_3 | Bangalore | Telangana   | India   | 2023-03-24        | True      | 3          | 3    | 3          |\n",
    "| 4           | Customer_4 | Hyderabad | Telangana   | India   | 2023-06-04        | False     | 1          | 1    | 1          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cumulative Aggregations\n",
    "\n",
    "Cumulative aggregations allow us to compute running totals or other cumulative metrics over a set of rows. This is done using window functions with `rowsBetween` or `rangeBetween`.\n",
    "\n",
    "### Example: Cumulative Sum on Customer Data\n",
    "\n",
    "Let's compute the cumulative sum of `customer_id` within each city.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# Define a window partitioned by city and ordered by customer_id\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(\"customer_id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Add cumulative sum\n",
    "df_with_cumsum = df.withColumn(\"cumulative_sum\", _sum(\"customer_id\").over(window_spec))\n",
    "\n",
    "df_with_cumsum.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| customer_id | name       | city      | state       | country | registration_date | is_active | cumulative_sum |\n",
    "|-------------|------------|-----------|-------------|---------|-------------------|-----------|----------------|\n",
    "| 0           | Customer_0 | Pune      | West Bengal | India   | 2023-10-10        | True      | 0              |\n",
    "| 1           | Customer_1 | Bangalore | Gujarat     | India   | 2023-10-19        | False     | 1              |\n",
    "| 2           | Customer_2 | Bangalore | Karnataka   | India   | 2023-02-10        | True      | 3              |\n",
    "| 3           | Customer_3 | Bangalore | Telangana   | India   | 2023-03-24        | True      | 6              |\n",
    "| 4           | Customer_4 | Hyderabad | Telangana   | India   | 2023-06-04        | False     | 4              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pivoting Data\n",
    "\n",
    "Pivoting data allows us to transform data from long to wide format, making it easier to analyze and visualize.\n",
    "\n",
    "### Example: Pivoting Customer Data by City\n",
    "\n",
    "Let's pivot the customer data by `city` and count the number of customers in each city.\n",
    "\n",
    "```python\n",
    "pivoted_df = df.groupBy(\"city\").pivot(\"state\").count()\n",
    "pivoted_df.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| city      | Gujarat | Karnataka | Maharashtra | Telangana | West Bengal |\n",
    "|-----------|---------|-----------|-------------|-----------|-------------|\n",
    "| Pune      | 0       | 0         | 0           | 0         | 1           |\n",
    "| Bangalore | 1       | 1         | 1           | 2         | 0           |\n",
    "| Hyderabad | 0       | 1         | 0           | 2         | 0           |\n",
    "| Ahmedabad | 0       | 0         | 0           | 0         | 0           |\n",
    "| Chennai   | 1       | 0         | 0           | 0         | 0           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, we explored various types of aggregations in PySpark, including simple aggregations, grouped aggregations, window functions, cumulative aggregations, and pivoting data. These operations are essential for summarizing and transforming large datasets in big data processing.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Simple Aggregations**: Use functions like `count`, `sum`, `avg`, `min`, and `max` for basic data summarization.\n",
    "- **Grouped Aggregations**: Use `groupBy` to perform aggregations on subsets of data.\n",
    "- **Window Functions**: Use window functions for advanced row-level calculations like ranking and cumulative sums.\n",
    "- **Cumulative Aggregations**: Use `rowsBetween` or `rangeBetween` to compute running totals.\n",
    "- **Pivoting Data**: Use `pivot` to transform data from long to wide format.\n",
    "\n",
    "By mastering these aggregation techniques, you can efficiently process and analyze large datasets in PySpark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}