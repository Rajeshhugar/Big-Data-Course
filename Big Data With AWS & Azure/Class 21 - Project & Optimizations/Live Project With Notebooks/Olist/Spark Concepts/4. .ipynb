{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Functions in PySpark\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Window Functions](#1-introduction-to-window-functions)\n",
    "2. [Key Concepts in Window Functions](#2-key-concepts-in-window-functions)\n",
    "3. [Rank](#3-rank)\n",
    "4. [Dense Rank](#4-dense-rank)\n",
    "5. [Row Number](#5-row-number)\n",
    "6. [Lead](#6-lead)\n",
    "7. [Lag](#7-lag)\n",
    "8. [Summary](#8-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Window Functions\n",
    "\n",
    "Window functions in PySpark are used to perform calculations across a set of rows that are related to the current row. \n",
    "Unlike simple aggregations, window functions allow you to define a \"window\" of rows over which the calculation is performed. \n",
    "This is particularly useful for tasks like ranking, cumulative sums, and comparing rows.\n",
    "\n",
    "### Why are Window Functions Important?\n",
    "- **Ranking**: Assign ranks to rows based on specific criteria.\n",
    "- **Cumulative Calculations**: Compute running totals, averages, etc.\n",
    "- **Row Comparison**: Compare current rows with previous or next rows using `lead` and `lag`.\n",
    "- **Data Analysis**: Perform advanced data analysis without reshaping the data.\n",
    "\n",
    "### Key Components of Window Functions\n",
    "- **Partitioning**: Divides the data into groups (e.g., by country).\n",
    "- **Ordering**: Specifies the order of rows within each partition.\n",
    "- **Window Frame**: Defines the range of rows to include in the calculation (e.g., all previous rows, current row, etc.).\n",
    "\n",
    "```\n",
    "┌───────────────┐\n",
    "│  Partition 1  │\n",
    "│  Row 1        │\n",
    "│  Row 2        │\n",
    "│  Row 3        │\n",
    "└───────────────┘\n",
    "┌───────────────┐\n",
    "│  Partition 2  │\n",
    "│  Row 1        │\n",
    "│  Row 2        │\n",
    "└───────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Concepts in Window Functions\n",
    "\n",
    "Before diving into specific window functions, let's understand the key concepts:\n",
    "\n",
    "### Partitioning\n",
    "Partitioning divides the dataset into groups based on a column (e.g., `country`). Each group is processed independently.\n",
    "\n",
    "### Ordering\n",
    "Ordering specifies the order of rows within each partition. For example, you can order rows by `invoicevalue` in descending order.\n",
    "\n",
    "### Window Frame\n",
    "The window frame defines the range of rows to include in the calculation. Common frames include:\n",
    "- **Unbounded Preceding**: All rows from the start of the partition to the current row.\n",
    "- **Current Row**: Only the current row.\n",
    "- **Unbounded Following**: All rows from the current row to the end of the partition.\n",
    "\n",
    "```\n",
    "┌───────────────┐\n",
    "│  Partition 1  │\n",
    "│  Row 1        │  ◄── Window Frame\n",
    "│  Row 2        │\n",
    "│  Row 3        │\n",
    "└───────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rank\n",
    "\n",
    "The `rank` function assigns a rank to each row within a partition. Rows with the same value receive the same rank, and the next rank is skipped.\n",
    "\n",
    "### Example: Rank Customers by Invoice Value\n",
    "\n",
    "Let's rank customers within each country based on their `invoicevalue`.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate()\n",
    "\n",
    "# Load customer data\n",
    "df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(df[\"invoicevalue\"].desc())\n",
    "\n",
    "# Add rank column\n",
    "df_with_rank = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Show results\n",
    "df_with_rank.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| country | invoicevalue | rank |\n",
    "|---------|--------------|------|\n",
    "| USA     | 1000         | 1    |\n",
    "| USA     | 1000         | 1    |\n",
    "| USA     | 800          | 3    |\n",
    "| UK      | 1200         | 1    |\n",
    "| UK      | 900          | 2    |\n",
    "\n",
    "### Key Points:\n",
    "- **Same Value, Same Rank**: Rows with the same `invoicevalue` receive the same rank.\n",
    "- **Rank Skipping**: The next rank is skipped after ties (e.g., rank 1, 1, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dense Rank\n",
    "\n",
    "The `dense_rank` function is similar to `rank`, but it does not skip ranks after ties. Rows with the same value receive the same rank, and the next rank is not skipped.\n",
    "\n",
    "### Example: Dense Rank Customers by Invoice Value\n",
    "\n",
    "Let's dense rank customers within each country based on their `invoicevalue`.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "# Add dense rank column\n",
    "df_with_dense_rank = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "\n",
    "# Show results\n",
    "df_with_dense_rank.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| country | invoicevalue | dense_rank |\n",
    "|---------|--------------|------------|\n",
    "| USA     | 1000         | 1          |\n",
    "| USA     | 1000         | 1          |\n",
    "| USA     | 800          | 2          |\n",
    "| UK      | 1200         | 1          |\n",
    "| UK      | 900          | 2          |\n",
    "\n",
    "### Key Points:\n",
    "- **Same Value, Same Rank**: Rows with the same `invoicevalue` receive the same rank.\n",
    "- **No Rank Skipping**: The next rank is not skipped after ties (e.g., rank 1, 1, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Row Number\n",
    "\n",
    "The `row_number` function assigns a unique sequential number to each row within a partition, starting from 1. Unlike `rank` and `dense_rank`, `row_number` does not handle ties.\n",
    "\n",
    "### Example: Row Number for Customers by Invoice Value\n",
    "\n",
    "Let's assign row numbers to customers within each country based on their `invoicevalue`.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Add row number column\n",
    "df_with_row_number = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Show results\n",
    "df_with_row_number.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| country | invoicevalue | row_number |\n",
    "|---------|--------------|------------|\n",
    "| USA     | 1000         | 1          |\n",
    "| USA     | 1000         | 2          |\n",
    "| USA     | 800          | 3          |\n",
    "| UK      | 1200         | 1          |\n",
    "| UK      | 900          | 2          |\n",
    "\n",
    "### Key Points:\n",
    "- **Unique Numbers**: Each row gets a unique number, even if values are the same.\n",
    "- **No Handling of Ties**: Ties are not handled; each row gets a distinct number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lead\n",
    "\n",
    "The `lead` function allows you to access the value of a column in the next row within the same partition. This is useful for comparing the current row with the next row.\n",
    "\n",
    "### Example: Lead for Invoice Value\n",
    "\n",
    "Let's compare the current `invoicevalue` with the next row's `invoicevalue`.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "# Add lead column\n",
    "df_with_lead = df.withColumn(\"next_invoicevalue\", lead(\"invoicevalue\").over(window_spec))\n",
    "\n",
    "# Show results\n",
    "df_with_lead.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| country | invoicevalue | next_invoicevalue |\n",
    "|---------|--------------|-------------------|\n",
    "| USA     | 1000         | 1000              |\n",
    "| USA     | 1000         | 800               |\n",
    "| USA     | 800          | null              |\n",
    "| UK      | 1200         | 900               |\n",
    "| UK      | 900          | null              |\n",
    "\n",
    "### Key Points:\n",
    "- **Next Row Value**: Accesses the value of the next row.\n",
    "- **Null for Last Row**: The last row in each partition will have `null` for the lead value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lag\n",
    "\n",
    "The `lag` function allows you to access the value of a column in the previous row within the same partition. This is useful for comparing the current row with the previous row.\n",
    "\n",
    "### Example: Lag for Invoice Value\n",
    "\n",
    "Let's compare the current `invoicevalue` with the previous row's `invoicevalue`.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Add lag column\n",
    "df_with_lag = df.withColumn(\"previous_invoicevalue\", lag(\"invoicevalue\").over(window_spec))\n",
    "\n",
    "# Show results\n",
    "df_with_lag.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "| country | invoicevalue | previous_invoicevalue |\n",
    "|---------|--------------|-----------------------|\n",
    "| USA     | 1000         | null                  |\n",
    "| USA     | 1000         | 1000                  |\n",
    "| USA     | 800          | 1000                  |\n",
    "| UK      | 1200         | null                  |\n",
    "| UK      | 900          | 1200                  |\n",
    "\n",
    "### Key Points:\n",
    "- **Previous Row Value**: Accesses the value of the previous row.\n",
    "- **Null for First Row**: The first row in each partition will have `null` for the lag value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we explored **Window Functions** in PySpark, including `rank`, `dense_rank`, `row_number`, `lead`, and `lag`. These functions are essential for advanced data analysis, allowing you to perform calculations across related rows.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Rank**: Assigns ranks with skips after ties.\n",
    "- **Dense Rank**: Assigns ranks without skips after ties.\n",
    "- **Row Number**: Assigns unique sequential numbers to rows.\n",
    "- **Lead**: Accesses the value of the next row.\n",
    "- **Lag**: Accesses the value of the previous row.\n",
    "\n",
    "By mastering these window functions, you can perform complex data analysis tasks efficiently in PySpark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}